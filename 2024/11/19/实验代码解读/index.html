<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>实验代码解读 | 零度Blog</title><meta name="author" content="tfy"><meta name="copyright" content="tfy"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="实验相关细节实验代码import copyimport globimport mathimport osimport randomfrom collections import namedtuplefrom functools import partialfrom multiprocessing import cpu_countfrom pathlib import Pathimport Augm">
<meta property="og:type" content="article">
<meta property="og:title" content="实验代码解读">
<meta property="og:url" content="https://nil0330.github.io/2024/11/19/%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/index.html">
<meta property="og:site_name" content="零度Blog">
<meta property="og:description" content="实验相关细节实验代码import copyimport globimport mathimport osimport randomfrom collections import namedtuplefrom functools import partialfrom multiprocessing import cpu_countfrom pathlib import Pathimport Augm">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411141618792.png">
<meta property="article:published_time" content="2024-11-19T08:44:21.000Z">
<meta property="article:modified_time" content="2024-12-20T05:19:00.736Z">
<meta property="article:author" content="tfy">
<meta property="article:tag" content="源码解读">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411141618792.png"><link rel="shortcut icon" href="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411141621135.png"><link rel="canonical" href="https://nil0330.github.io/2024/11/19/%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '实验代码解读',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  isShuoshuo: false
}</script><meta name="generator" content="Hexo 7.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411141618792.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://raw.githubusercontent.com/nil0330/PicGo1/main/202411201439551.webp);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">零度Blog</span></a><a class="nav-page-title" href="/"><span class="site-name">实验代码解读</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa fa-graduation-cap"></i><span> 博文</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/categories/"><i class="fa-fw fa fa-archive"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/archives/"><i class="fa-fw fa fa-folder-open"></i><span> 归档</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-list"></i><span> 生活</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/shuoshuo/"><i class="fa-fw fa fa-comments-o"></i><span> 分享</span></a></li><li><a class="site-page child" href="/photos/"><i class="fa-fw fa fa-camera-retro"></i><span> 相册</span></a></li><li><a class="site-page child" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/links/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/comment/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于笔者</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">实验代码解读</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-11-19T08:44:21.000Z" title="发表于 2024-11-19 16:44:21">2024-11-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-12-20T05:19:00.736Z" title="更新于 2024-12-20 13:19:00">2024-12-20</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="实验相关细节"><a href="#实验相关细节" class="headerlink" title="实验相关细节"></a>实验相关细节</h1><h2 id="实验代码"><a href="#实验代码" class="headerlink" title="实验代码"></a>实验代码</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> cpu_count</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> Augmentor</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms.functional <span class="keyword">as</span> TF</span><br><span class="line"><span class="keyword">from</span> accelerate <span class="keyword">import</span> Accelerator</span><br><span class="line"><span class="keyword">from</span> datasets.get_dataset <span class="keyword">import</span> dataset</span><br><span class="line"><span class="keyword">from</span> einops <span class="keyword">import</span> rearrange, reduce</span><br><span class="line"><span class="keyword">from</span> einops.layers.torch <span class="keyword">import</span> Rearrange</span><br><span class="line"><span class="keyword">from</span> ema_pytorch <span class="keyword">import</span> EMA</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> einsum, nn</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms <span class="keyword">as</span> T</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> tqdm.auto <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line">ModelResPrediction = namedtuple(</span><br><span class="line">    <span class="string">&#x27;ModelResPrediction&#x27;</span>, [<span class="string">&#x27;pred_res&#x27;</span>, <span class="string">&#x27;pred_noise&#x27;</span>, <span class="string">&#x27;pred_x_start&#x27;</span>])</span><br><span class="line"><span class="comment"># 定义了名为ModelResPrediction的nametuple包含了三个字段</span></span><br><span class="line"><span class="comment"># helpers functions</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">SEED</span>):</span><br><span class="line">    <span class="comment"># initialize random seed</span></span><br><span class="line">    torch.manual_seed(SEED)</span><br><span class="line">    torch.cuda.manual_seed_all(SEED)</span><br><span class="line">    np.random.seed(SEED)</span><br><span class="line">    random.seed(SEED)</span><br><span class="line"><span class="comment"># 设置随机种子确保多次运行代码时能够获得相同的随机效果</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">exists</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> x <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">default</span>(<span class="params">val, d</span>):</span><br><span class="line">    <span class="keyword">if</span> exists(val):</span><br><span class="line">        <span class="keyword">return</span> val</span><br><span class="line">    <span class="keyword">return</span> d() <span class="keyword">if</span> <span class="built_in">callable</span>(d) <span class="keyword">else</span> d</span><br><span class="line"><span class="comment"># 目的时根据传入的参数val是否存在来返回一个默认值</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">identity</span>(<span class="params">t, *args, **kwargs</span>):</span><br><span class="line">    <span class="keyword">return</span> t</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cycle</span>(<span class="params">dl</span>):</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> dl:</span><br><span class="line">            <span class="keyword">yield</span> data</span><br><span class="line"></span><br><span class="line"><span class="comment"># yield可以暂停函数的执行,和return不同的是返回值不会停止函数的运行而是保存当前的状态,形成一个迭代器的效果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">has_int_squareroot</span>(<span class="params">num</span>):</span><br><span class="line">    <span class="keyword">return</span> (math.sqrt(num) ** <span class="number">2</span>) == num</span><br><span class="line"><span class="comment"># 判断是不是一个完全平方数</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">num_to_groups</span>(<span class="params">num, divisor</span>):</span><br><span class="line">    groups = num // divisor</span><br><span class="line">    remainder = num % divisor</span><br><span class="line">    arr = [divisor] * groups</span><br><span class="line">    <span class="keyword">if</span> remainder &gt; <span class="number">0</span>:</span><br><span class="line">        arr.append(remainder)</span><br><span class="line">    <span class="keyword">return</span> arr</span><br><span class="line"><span class="comment"># 作用是将 num 分成若干个大小为 divisor 的完整组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># normalization functions</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_to_neg_one_to_one</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(img, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">return</span> [img[k] * <span class="number">2</span> - <span class="number">1</span> <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(img))]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> img * <span class="number">2</span> - <span class="number">1</span></span><br><span class="line"><span class="comment"># 将输入的图片归一化到-1到1之间</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unnormalize_to_zero_to_one</span>(<span class="params">img</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(img, <span class="built_in">list</span>):</span><br><span class="line">        <span class="keyword">return</span> [(img[k] + <span class="number">1</span>) * <span class="number">0.5</span> <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(img))]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> (img + <span class="number">1</span>) * <span class="number">0.5</span></span><br><span class="line"><span class="comment"># 将输入的图片归一化到[0,1]之间</span></span><br><span class="line"><span class="comment"># small helper modules</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Residual</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fn = fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, *args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fn(x, *args, **kwargs) + x</span><br><span class="line"><span class="comment"># 声明一个Residua类继承了nn.Module,接收一个函数fn保存在self.fn</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Upsample</span>(<span class="params">dim, dim_out=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        nn.Upsample(scale_factor=<span class="number">2</span>, mode=<span class="string">&#x27;nearest&#x27;</span>),</span><br><span class="line">        nn.Conv2d(dim, default(dim_out, dim), <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line"><span class="comment"># 返回一个sequential容器模块,将多个层按照顺序堆叠</span></span><br><span class="line"><span class="comment"># scale_factor=2：意味着将输入的空间维度（宽和高）扩展为原来的 2 倍。例如，如果输入图像是 32x32，输出将变为 64x64。</span></span><br><span class="line"><span class="comment"># mode=&#x27;nearest&#x27;：采用最近邻插值方法进行上采样。最近邻插值方法简单而高效，它通过复制最接近的像素来扩展图像。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">Downsample</span>(<span class="params">dim, dim_out=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">return</span> nn.Conv2d(dim, default(dim_out, dim), <span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WeightStandardizedConv2d</span>(nn.Conv2d):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1903.10520</span></span><br><span class="line"><span class="string">    weight standardization purportedly works synergistically with group normalization</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        eps = <span class="number">1e-5</span> <span class="keyword">if</span> x.dtype == torch.float32 <span class="keyword">else</span> <span class="number">1e-3</span></span><br><span class="line"><span class="comment"># eps 是一个小的常数，用于防止在计算标准差时出现除以零的情况。</span></span><br><span class="line">        weight = <span class="variable language_">self</span>.weight</span><br><span class="line">        mean = reduce(weight, <span class="string">&#x27;o ... -&gt; o 1 1 1&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        var = reduce(weight, <span class="string">&#x27;o ... -&gt; o 1 1 1&#x27;</span>,</span><br><span class="line">                     partial(torch.var, unbiased=<span class="literal">False</span>))</span><br><span class="line">        normalized_weight = (weight - mean) * (var + eps).rsqrt()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> F.conv2d(x, normalized_weight, <span class="variable language_">self</span>.bias, <span class="variable language_">self</span>.stride, <span class="variable language_">self</span>.padding, <span class="variable language_">self</span>.dilation, <span class="variable language_">self</span>.groups)</span><br><span class="line"><span class="comment"># 自定义的卷积层,实现了权重标准化,卷积核的权重在每次前向传播时进行标准化,以减少梯度更新中的不平衡问题.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.g = nn.Parameter(torch.ones(<span class="number">1</span>, dim, <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        eps = <span class="number">1e-5</span> <span class="keyword">if</span> x.dtype == torch.float32 <span class="keyword">else</span> <span class="number">1e-3</span></span><br><span class="line">        var = torch.var(x, dim=<span class="number">1</span>, unbiased=<span class="literal">False</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        mean = torch.mean(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> (x - mean) * (var + eps).rsqrt() * <span class="variable language_">self</span>.g</span><br><span class="line"><span class="comment"># 实现了层归一化操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PreNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, fn</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fn = fn</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.fn(x)</span><br><span class="line"><span class="comment"># 一个自定义的神经网络模块，结合了归一化层和传入的函数fn</span></span><br><span class="line"><span class="comment"># sinusoidal positional embeds</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SinusoidalPosEmb</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.dim = dim</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        device = x.device</span><br><span class="line">        half_dim = <span class="variable language_">self</span>.dim // <span class="number">2</span></span><br><span class="line">        emb = math.log(<span class="number">10000</span>) / (half_dim - <span class="number">1</span>)</span><br><span class="line">        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)</span><br><span class="line">        emb = x[:, <span class="literal">None</span>] * emb[<span class="literal">None</span>, :]</span><br><span class="line">        emb = torch.cat((emb.sin(), emb.cos()), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> emb</span><br><span class="line"><span class="comment"># 正弦波位置编码</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RandomOrLearnedSinusoidalPosEmb</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; following @crowsonkb &#x27;s lead with random (learned optional) sinusoidal pos emb &quot;&quot;&quot;</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, is_random=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> (dim % <span class="number">2</span>) == <span class="number">0</span></span><br><span class="line">        half_dim = dim // <span class="number">2</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = nn.Parameter(torch.randn(</span><br><span class="line">            half_dim), requires_grad=<span class="keyword">not</span> is_random)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = rearrange(x, <span class="string">&#x27;b -&gt; b 1&#x27;</span>)</span><br><span class="line">        freqs = x * rearrange(<span class="variable language_">self</span>.weights, <span class="string">&#x27;d -&gt; 1 d&#x27;</span>) * <span class="number">2</span> * math.pi</span><br><span class="line">        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim=-<span class="number">1</span>)</span><br><span class="line">        fouriered = torch.cat((x, fouriered), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> fouriered</span><br><span class="line"><span class="comment"># 是一个自定义的Pytorch模块，用于生成位置编码，随机或可学习的正弦波位置编码</span></span><br><span class="line"><span class="comment"># building block modules</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Block</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, dim_out, groups=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.proj = WeightStandardizedConv2d(dim, dim_out, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.GroupNorm(groups, dim_out)</span><br><span class="line">        <span class="variable language_">self</span>.act = nn.SiLU()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, scale_shift=<span class="literal">None</span></span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exists(scale_shift):</span><br><span class="line">            scale, shift = scale_shift</span><br><span class="line">            x = x * (scale + <span class="number">1</span>) + shift</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.act(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义的神经网络模块，构造函数接收三个参数：dim、dim_out、groups(用于分组数)</span></span><br><span class="line"><span class="comment"># 使用了一个加权标准化卷积、groupNorm以及Silu激活函数来组成</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResnetBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, dim_out, *, time_emb_dim=<span class="literal">None</span>, groups=<span class="number">8</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.mlp = nn.Sequential(</span><br><span class="line">            nn.SiLU(),</span><br><span class="line">            nn.Linear(time_emb_dim, dim_out * <span class="number">2</span>)</span><br><span class="line">        ) <span class="keyword">if</span> exists(time_emb_dim) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="comment"># 构造函数接收的参数，dim输入的特征通道数、dim——out输出的特征通道数、</span></span><br><span class="line"><span class="comment"># time-emb-dim如果存在时间嵌入的维度，group指的groupNorm的分组数 </span></span><br><span class="line"><span class="comment"># 如果提供了time-emb-dim则构建一个MLP，nn.SiLU() 激活函数。</span></span><br><span class="line"><span class="comment"># nn.Linear(time_emb_dim, dim_out * 2) 全连接层，将时间嵌入映射到 dim_out * 2 的维度。</span></span><br><span class="line">        <span class="variable language_">self</span>.block1 = Block(dim, dim_out, groups=groups)</span><br><span class="line">        <span class="variable language_">self</span>.block2 = Block(dim_out, dim_out, groups=groups)</span><br><span class="line">        <span class="variable language_">self</span>.res_conv = nn.Conv2d(</span><br><span class="line">            dim, dim_out, <span class="number">1</span>) <span class="keyword">if</span> dim != dim_out <span class="keyword">else</span> nn.Identity()</span><br><span class="line"><span class="comment"># 这是一个残差链接的卷积，如果输入和输出不相同则通过1*1卷积来匹配他们的维度负责直接跳过进行相加</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time_emb=<span class="literal">None</span></span>):</span><br><span class="line"></span><br><span class="line">        scale_shift = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> exists(<span class="variable language_">self</span>.mlp) <span class="keyword">and</span> exists(time_emb):</span><br><span class="line">            time_emb = <span class="variable language_">self</span>.mlp(time_emb)</span><br><span class="line">            time_emb = rearrange(time_emb, <span class="string">&#x27;b c -&gt; b c 1 1&#x27;</span>)</span><br><span class="line">            <span class="comment"># 将时间维度和空间维度对齐，形状保证一样</span></span><br><span class="line">            scale_shift = time_emb.chunk(<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 将时间维度按照维度为1分为两个部分,一个用于缩放,一个用于平移</span></span><br><span class="line">            <span class="comment"># 如果时间嵌入存在并且被mlp定义</span></span><br><span class="line">        h = <span class="variable language_">self</span>.block1(x, scale_shift=scale_shift)</span><br><span class="line"></span><br><span class="line">        h = <span class="variable language_">self</span>.block2(h)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> h + <span class="variable language_">self</span>.res_conv(x)</span><br><span class="line"><span class="comment"># 最后使用残差链接</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearAttention</span>(nn.Module):</span><br><span class="line">    <span class="comment"># 线性注意力，可以通过线性化注意力计算来提高效率，能够避免显示地计算完整的注意力矩阵</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">4</span>, dim_head=<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line">        <span class="comment"># 一个缩放因子,用于在计算注意力时避免过大的值,这里是dim_head ** -0.5,用于标准化查询向量</span></span><br><span class="line">        <span class="variable language_">self</span>.heads = heads</span><br><span class="line">        hidden_dim = dim_head * heads</span><br><span class="line">        <span class="comment"># 每层的隐藏维度大小</span></span><br><span class="line">        <span class="variable language_">self</span>.to_qkv = nn.Conv2d(dim, hidden_dim * <span class="number">3</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 用于从输入中生成q,k,v</span></span><br><span class="line">        <span class="variable language_">self</span>.to_out = nn.Sequential(</span><br><span class="line">            nn.Conv2d(hidden_dim, dim, <span class="number">1</span>),</span><br><span class="line">            LayerNorm(dim)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 用于将注意力计算后的输出映射回输入的维度,并进行归一化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.shape</span><br><span class="line">        <span class="comment"># 分别获得批量大小、c是输入的通道数、h和w是输入的高和宽</span></span><br><span class="line">        qkv = <span class="variable language_">self</span>.to_qkv(x).chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(</span><br><span class="line">            t, <span class="string">&#x27;b (h c) x y -&gt; b h c (x y)&#x27;</span>, h=<span class="variable language_">self</span>.heads), qkv)</span><br><span class="line">        <span class="comment"># 将qkv的形状进行重塑，方便后续的计算</span></span><br><span class="line">        q = q.softmax(dim=-<span class="number">2</span>)</span><br><span class="line">        k = k.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        q = q * <span class="variable language_">self</span>.scale</span><br><span class="line">        v = v / (h * w)</span><br><span class="line"></span><br><span class="line">        context = torch.einsum(<span class="string">&#x27;b h d n, b h e n -&gt; b h d e&#x27;</span>, k, v)</span><br><span class="line"><span class="comment"># 使用爱因斯坦求和约定（einsum）计算上下文向量：context 是键和值的内积。</span></span><br><span class="line"><span class="comment"># 这个操作类似于传统的自注意力机制中计算 QK^T 的过程，但是在这里，键和值的计算被线性化处理</span></span><br><span class="line">        out = torch.einsum(<span class="string">&#x27;b h d e, b h d n -&gt; b h e n&#x27;</span>, context, q)</span><br><span class="line">        <span class="comment"># 这里通过 einsum 计算上下文和查询的乘积，得到最终的输出。</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h c (x y) -&gt; b (h c) x y&#x27;</span>,</span><br><span class="line">                        h=<span class="variable language_">self</span>.heads, x=h, y=w)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.to_out(out)</span><br><span class="line"><span class="comment"># 使用 rearrange 将输出的形状调整为 [batch_size, heads * dim_head, height, width]。</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, heads=<span class="number">4</span>, dim_head=<span class="number">32</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.scale = dim_head ** -<span class="number">0.5</span></span><br><span class="line">        <span class="variable language_">self</span>.heads = heads</span><br><span class="line">        hidden_dim = dim_head * heads</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.to_qkv = nn.Conv2d(dim, hidden_dim * <span class="number">3</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.to_out = nn.Conv2d(hidden_dim, dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, h, w = x.shape</span><br><span class="line">        qkv = <span class="variable language_">self</span>.to_qkv(x).chunk(<span class="number">3</span>, dim=<span class="number">1</span>)</span><br><span class="line">        q, k, v = <span class="built_in">map</span>(<span class="keyword">lambda</span> t: rearrange(</span><br><span class="line">            t, <span class="string">&#x27;b (h c) x y -&gt; b h c (x y)&#x27;</span>, h=<span class="variable language_">self</span>.heads), qkv)</span><br><span class="line"></span><br><span class="line">        q = q * <span class="variable language_">self</span>.scale</span><br><span class="line"></span><br><span class="line">        sim = einsum(<span class="string">&#x27;b h d i, b h d j -&gt; b h i j&#x27;</span>, q, k)</span><br><span class="line">        <span class="comment"># einsum(&#x27;b h d i, b h d j -&gt; b h i j&#x27;, q, k)：计算查询和键的点积，</span></span><br><span class="line">        <span class="comment"># 得到一个相似度矩阵 sim，表示每个查询和每个键之间的相似度（注意力分数）。</span></span><br><span class="line">        attn = sim.softmax(dim=-<span class="number">1</span>)</span><br><span class="line">        out = einsum(<span class="string">&#x27;b h i j, b h d j -&gt; b h i d&#x27;</span>, attn, v)</span><br><span class="line"><span class="comment"># einsum(&#x27;b h i j, b h d j -&gt; b h i d&#x27;, attn, v)：用注意力权重 attn 对值（v）</span></span><br><span class="line"><span class="comment"># 进行加权求和，得到最终的注意力输出。这个步骤的意思是将每个位置的值加权合成一个新的表示。</span></span><br><span class="line">        out = rearrange(out, <span class="string">&#x27;b h (x y) d -&gt; b (h d) x y&#x27;</span>, x=h, y=w)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.to_out(out)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Unet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        dim,</span></span><br><span class="line"><span class="params">        init_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        out_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dim_mults=(<span class="params"><span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span></span>),</span></span><br><span class="line"><span class="params">        channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">        self_condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        resnet_block_groups=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">        learned_variance=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        learned_sinusoidal_cond=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        random_fourier_features=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        learned_sinusoidal_dim=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">        condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        input_condition=<span class="literal">False</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># determine dimensions</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line">        <span class="variable language_">self</span>.self_condition = self_condition</span><br><span class="line">        input_channels = channels + channels * \</span><br><span class="line">            (<span class="number">1</span> <span class="keyword">if</span> self_condition <span class="keyword">else</span> <span class="number">0</span>) + channels * \</span><br><span class="line">            (<span class="number">1</span> <span class="keyword">if</span> condition <span class="keyword">else</span> <span class="number">0</span>) + channels * (<span class="number">1</span> <span class="keyword">if</span> input_condition <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 计算最终输入的通道数，包括原始图像通道数和附件的条件信息通道数</span></span><br><span class="line">        init_dim = default(init_dim, dim)</span><br><span class="line">        <span class="variable language_">self</span>.init_conv = nn.Conv2d(input_channels, init_dim, <span class="number">7</span>, padding=<span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 初始化卷积层</span></span><br><span class="line">        dims = [init_dim, *<span class="built_in">map</span>(<span class="keyword">lambda</span> m: dim * m, dim_mults)]</span><br><span class="line">        <span class="comment"># 根据dim_mults来计算每个分辨率的维度.</span></span><br><span class="line">        in_out = <span class="built_in">list</span>(<span class="built_in">zip</span>(dims[:-<span class="number">1</span>], dims[<span class="number">1</span>:]))</span><br><span class="line">        <span class="comment"># 创建一个由每层输入和输出通道组成的元组列表，用于后续的编码器和解码器层。</span></span><br><span class="line">        <span class="comment"># zip是用来配对的,dims[:-1]出去最后一个元素,dims[1:]除去第一个元素</span></span><br><span class="line">        block_klass = partial(ResnetBlock, groups=resnet_block_groups)</span><br><span class="line">        <span class="comment"># 通过使用partial来预设一些某些参数来设置一个新的函数</span></span><br><span class="line">        <span class="comment"># time embeddings</span></span><br><span class="line"></span><br><span class="line">        time_dim = dim * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond <span class="keyword">or</span> random_fourier_features</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.random_or_learned_sinusoidal_cond:</span><br><span class="line">            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(</span><br><span class="line">                learned_sinusoidal_dim, random_fourier_features)</span><br><span class="line">            fourier_dim = learned_sinusoidal_dim + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sinu_pos_emb = SinusoidalPosEmb(dim)</span><br><span class="line">            fourier_dim = dim</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.time_mlp = nn.Sequential(</span><br><span class="line">            sinu_pos_emb,</span><br><span class="line">            nn.Linear(fourier_dim, time_dim),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Linear(time_dim, time_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># layers</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.downs = nn.ModuleList([])</span><br><span class="line">        <span class="variable language_">self</span>.ups = nn.ModuleList([])</span><br><span class="line">        <span class="comment"># self.downs 和 self.ups：分别是下采样和上采样的模块列表</span></span><br><span class="line">        num_resolutions = <span class="built_in">len</span>(in_out)</span><br><span class="line">        <span class="comment"># in_out 是一个由每层输入和输出通道组成的列表。</span></span><br><span class="line">        <span class="comment"># num_resolutions 计算的是 U-Net 网络中的分辨率数量。由于 in_out 是由每层的输入和输出通道数对组成的列表</span></span><br><span class="line">        <span class="comment"># 即网络中下采样和上采样的层数。</span></span><br><span class="line">        <span class="keyword">for</span> ind, (dim_in, dim_out) <span class="keyword">in</span> <span class="built_in">enumerate</span>(in_out):</span><br><span class="line">            <span class="comment"># ind是索引，(dim_in, dim_out)是维度对</span></span><br><span class="line">            is_last = ind &gt;= (num_resolutions - <span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 判断是不是最后一个模块</span></span><br><span class="line">            <span class="variable language_">self</span>.downs.append(nn.ModuleList([</span><br><span class="line">                block_klass(dim_in, dim_in, time_emb_dim=time_dim),</span><br><span class="line">                block_klass(dim_in, dim_in, time_emb_dim=time_dim),</span><br><span class="line">                Residual(PreNorm(dim_in, LinearAttention(dim_in))),</span><br><span class="line">                Downsample(dim_in, dim_out) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span> nn.Conv2d(</span><br><span class="line">                    dim_in, dim_out, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">        mid_dim = dims[-<span class="number">1</span>]</span><br><span class="line">        <span class="variable language_">self</span>.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)</span><br><span class="line">        <span class="variable language_">self</span>.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))</span><br><span class="line">        <span class="variable language_">self</span>.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ind, (dim_in, dim_out) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(in_out)):</span><br><span class="line">            is_last = ind == (<span class="built_in">len</span>(in_out) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.ups.append(nn.ModuleList([</span><br><span class="line">                block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),</span><br><span class="line">                block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),</span><br><span class="line">                Residual(PreNorm(dim_out, LinearAttention(dim_out))),</span><br><span class="line">                Upsample(dim_out, dim_in) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span> nn.Conv2d(</span><br><span class="line">                    dim_out, dim_in, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            ]))</span><br><span class="line"></span><br><span class="line">        default_out_dim = channels * (<span class="number">1</span> <span class="keyword">if</span> <span class="keyword">not</span> learned_variance <span class="keyword">else</span> <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.out_dim = default(out_dim, default_out_dim)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.final_res_block = block_klass(dim * <span class="number">2</span>, dim, time_emb_dim=time_dim)</span><br><span class="line">        <span class="variable language_">self</span>.final_conv = nn.Conv2d(dim, <span class="variable language_">self</span>.out_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time, x_self_cond=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_condition:</span><br><span class="line">            x_self_cond = default(x_self_cond, <span class="keyword">lambda</span>: torch.zeros_like(x))</span><br><span class="line">            x = torch.cat((x_self_cond, x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.init_conv(x)</span><br><span class="line">        r = x.clone()</span><br><span class="line">        <span class="comment"># r用于后续的网络的残差连接</span></span><br><span class="line">        t = <span class="variable language_">self</span>.time_mlp(time)</span><br><span class="line"></span><br><span class="line">        h = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> block1, block2, attn, downsample <span class="keyword">in</span> <span class="variable language_">self</span>.downs:</span><br><span class="line">            x = block1(x, t)</span><br><span class="line">            h.append(x)</span><br><span class="line"></span><br><span class="line">            x = block2(x, t)</span><br><span class="line">            x = attn(x)</span><br><span class="line">            h.append(x)</span><br><span class="line">        <span class="comment"># h列表保存每层处理后的特征，供后续的上采样部分使用</span></span><br><span class="line">            x = downsample(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="variable language_">self</span>.mid_block1(x, t)</span><br><span class="line">        x = <span class="variable language_">self</span>.mid_attn(x)</span><br><span class="line">        x = <span class="variable language_">self</span>.mid_block2(x, t)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> block1, block2, attn, upsample <span class="keyword">in</span> <span class="variable language_">self</span>.ups:</span><br><span class="line">            x = torch.cat((x, h.pop()), dim=<span class="number">1</span>)</span><br><span class="line">            x = block1(x, t)</span><br><span class="line"></span><br><span class="line">            x = torch.cat((x, h.pop()), dim=<span class="number">1</span>)</span><br><span class="line">            x = block2(x, t)</span><br><span class="line">            x = attn(x)</span><br><span class="line"></span><br><span class="line">            x = upsample(x)</span><br><span class="line"></span><br><span class="line">        x = torch.cat((x, r), dim=<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 形成残差连接</span></span><br><span class="line">        x = <span class="variable language_">self</span>.final_res_block(x, t)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.final_conv(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">UnetRes</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        dim,</span></span><br><span class="line"><span class="params">        init_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        out_dim=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        dim_mults=(<span class="params"><span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">8</span></span>),</span></span><br><span class="line"><span class="params">        channels=<span class="number">3</span>,</span></span><br><span class="line"><span class="params">        self_condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        resnet_block_groups=<span class="number">8</span>,</span></span><br><span class="line"><span class="params">        learned_variance=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        learned_sinusoidal_cond=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        random_fourier_features=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        learned_sinusoidal_dim=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">        share_encoder=<span class="number">1</span>,<span class="comment">#　控制 U-Net 的编码器部分如何在多个网络层或实例中共享。</span></span></span><br><span class="line"><span class="params">        condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        input_condition=<span class="literal">False</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.condition = condition</span><br><span class="line">        <span class="variable language_">self</span>.input_condition = input_condition</span><br><span class="line">        <span class="variable language_">self</span>.share_encoder = share_encoder</span><br><span class="line">        <span class="variable language_">self</span>.channels = channels</span><br><span class="line">        default_out_dim = channels * (<span class="number">1</span> <span class="keyword">if</span> <span class="keyword">not</span> learned_variance <span class="keyword">else</span> <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.out_dim = default(out_dim, default_out_dim)</span><br><span class="line">        <span class="variable language_">self</span>.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond <span class="keyword">or</span> random_fourier_features</span><br><span class="line">        <span class="variable language_">self</span>.self_condition = self_condition</span><br><span class="line"></span><br><span class="line">        <span class="comment"># determine dimensions</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.share_encoder == <span class="number">1</span>:</span><br><span class="line">            input_channels = channels + channels * \</span><br><span class="line">                (<span class="number">1</span> <span class="keyword">if</span> self_condition <span class="keyword">else</span> <span class="number">0</span>) + \</span><br><span class="line">                channels * (<span class="number">1</span> <span class="keyword">if</span> condition <span class="keyword">else</span> <span class="number">0</span>) + channels * \</span><br><span class="line">                (<span class="number">1</span> <span class="keyword">if</span> input_condition <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 这行代码计算了输入通道数 input_channels：</span></span><br><span class="line">            <span class="comment"># 基础通道数 channels。</span></span><br><span class="line">            <span class="comment"># 如果 self_condition 为真，增加 channels。</span></span><br><span class="line">            <span class="comment"># 如果 condition 为真，增加 channels。</span></span><br><span class="line">            <span class="comment"># 如果 input_condition 为真，增加 channels。</span></span><br><span class="line">            init_dim = default(init_dim, dim)</span><br><span class="line">            <span class="variable language_">self</span>.init_conv = nn.Conv2d(input_channels, init_dim, <span class="number">7</span>, padding=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">            dims = [init_dim, *<span class="built_in">map</span>(<span class="keyword">lambda</span> m: dim * m, dim_mults)]</span><br><span class="line">            <span class="comment"># map(lambda m: dim * m, dim_mults) 将 dim_mults 中的每个元素乘以 dim。</span></span><br><span class="line">            <span class="comment"># dims 是一个列表，包含初始维度 init_dim 和每个层级的维度。</span></span><br><span class="line">            in_out = <span class="built_in">list</span>(<span class="built_in">zip</span>(dims[:-<span class="number">1</span>], dims[<span class="number">1</span>:]))</span><br><span class="line"></span><br><span class="line">            block_klass = partial(ResnetBlock, groups=resnet_block_groups)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># time embeddings</span></span><br><span class="line"></span><br><span class="line">            time_dim = dim * <span class="number">4</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.random_or_learned_sinusoidal_cond:</span><br><span class="line">                sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(</span><br><span class="line">                    learned_sinusoidal_dim, random_fourier_features)</span><br><span class="line">                fourier_dim = learned_sinusoidal_dim + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                sinu_pos_emb = SinusoidalPosEmb(dim)</span><br><span class="line">                fourier_dim = dim</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.time_mlp = nn.Sequential(</span><br><span class="line">                sinu_pos_emb,</span><br><span class="line">                nn.Linear(fourier_dim, time_dim),</span><br><span class="line">                nn.GELU(),</span><br><span class="line">                nn.Linear(time_dim, time_dim)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># layers</span></span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.downs = nn.ModuleList([])</span><br><span class="line">            <span class="variable language_">self</span>.ups = nn.ModuleList([])</span><br><span class="line">            <span class="variable language_">self</span>.ups_no_skip = nn.ModuleList([])</span><br><span class="line">            <span class="comment"># 表示不使用跳跃连接的上采样块</span></span><br><span class="line">            num_resolutions = <span class="built_in">len</span>(in_out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> ind, (dim_in, dim_out) <span class="keyword">in</span> <span class="built_in">enumerate</span>(in_out):</span><br><span class="line">                is_last = ind &gt;= (num_resolutions - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.downs.append(nn.ModuleList([</span><br><span class="line">                    block_klass(dim_in, dim_in, time_emb_dim=time_dim),</span><br><span class="line">                    block_klass(dim_in, dim_in, time_emb_dim=time_dim),</span><br><span class="line">                    Residual(PreNorm(dim_in, LinearAttention(dim_in))),</span><br><span class="line">                    Downsample(dim_in, dim_out) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span> nn.Conv2d(</span><br><span class="line">                        dim_in, dim_out, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                ]))</span><br><span class="line"></span><br><span class="line">            mid_dim = dims[-<span class="number">1</span>]</span><br><span class="line">            <span class="variable language_">self</span>.mid_block1 = block_klass(</span><br><span class="line">                mid_dim, mid_dim, time_emb_dim=time_dim)</span><br><span class="line">            <span class="variable language_">self</span>.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))</span><br><span class="line">            <span class="variable language_">self</span>.mid_block2 = block_klass(</span><br><span class="line">                mid_dim, mid_dim, time_emb_dim=time_dim)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> ind, (dim_in, dim_out) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">reversed</span>(in_out)):</span><br><span class="line">                is_last = ind == (<span class="built_in">len</span>(in_out) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.ups.append(nn.ModuleList([</span><br><span class="line">                    block_klass(dim_out + dim_in, dim_out,</span><br><span class="line">                                time_emb_dim=time_dim),</span><br><span class="line">                    block_klass(dim_out + dim_in, dim_out,</span><br><span class="line">                                time_emb_dim=time_dim),</span><br><span class="line">                    Residual(PreNorm(dim_out, LinearAttention(dim_out))),</span><br><span class="line">                    Upsample(dim_out, dim_in) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span> nn.Conv2d(</span><br><span class="line">                        dim_out, dim_in, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                ]))</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.ups_no_skip.append(nn.ModuleList([</span><br><span class="line">                    block_klass(dim_out, dim_out, time_emb_dim=time_dim),</span><br><span class="line">                    block_klass(dim_out, dim_out, time_emb_dim=time_dim),</span><br><span class="line">                    Residual(PreNorm(dim_out, LinearAttention(dim_out))),</span><br><span class="line">                    Upsample(dim_out, dim_in) <span class="keyword">if</span> <span class="keyword">not</span> is_last <span class="keyword">else</span> nn.Conv2d(</span><br><span class="line">                        dim_out, dim_in, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">                ]))</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.final_res_block_1 = block_klass(</span><br><span class="line">                dim, dim, time_emb_dim=time_dim)</span><br><span class="line">            <span class="variable language_">self</span>.final_conv_1 = nn.Conv2d(dim, <span class="variable language_">self</span>.out_dim, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.final_res_block_2 = block_klass(</span><br><span class="line">                dim * <span class="number">2</span>, dim, time_emb_dim=time_dim)</span><br><span class="line">            <span class="variable language_">self</span>.final_conv_2 = nn.Conv2d(dim, <span class="variable language_">self</span>.out_dim, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.share_encoder == <span class="number">0</span>:</span><br><span class="line">            <span class="variable language_">self</span>.unet0 = Unet(dim,</span><br><span class="line">                              init_dim=init_dim,</span><br><span class="line">                              out_dim=out_dim,</span><br><span class="line">                              dim_mults=dim_mults,</span><br><span class="line">                              channels=channels,</span><br><span class="line">                              self_condition=self_condition,</span><br><span class="line">                              resnet_block_groups=resnet_block_groups,</span><br><span class="line">                              learned_variance=learned_variance,</span><br><span class="line">                              learned_sinusoidal_cond=learned_sinusoidal_cond,</span><br><span class="line">                              random_fourier_features=random_fourier_features,</span><br><span class="line">                              learned_sinusoidal_dim=learned_sinusoidal_dim,</span><br><span class="line">                              condition=condition,</span><br><span class="line">                              input_condition=input_condition)</span><br><span class="line">            <span class="variable language_">self</span>.unet1 = Unet(dim,</span><br><span class="line">                              init_dim=init_dim,</span><br><span class="line">                              out_dim=out_dim,</span><br><span class="line">                              dim_mults=dim_mults,</span><br><span class="line">                              channels=channels,</span><br><span class="line">                              self_condition=self_condition,</span><br><span class="line">                              resnet_block_groups=resnet_block_groups,</span><br><span class="line">                              learned_variance=learned_variance,</span><br><span class="line">                              learned_sinusoidal_cond=learned_sinusoidal_cond,</span><br><span class="line">                              random_fourier_features=random_fourier_features,</span><br><span class="line">                              learned_sinusoidal_dim=learned_sinusoidal_dim,</span><br><span class="line">                              condition=condition,</span><br><span class="line">                              input_condition=input_condition)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.share_encoder == -<span class="number">1</span>:</span><br><span class="line">            <span class="variable language_">self</span>.unet0 = Unet(dim,</span><br><span class="line">                              init_dim=init_dim,</span><br><span class="line">                              out_dim=out_dim,</span><br><span class="line">                              dim_mults=dim_mults,</span><br><span class="line">                              channels=channels,</span><br><span class="line">                              self_condition=self_condition,</span><br><span class="line">                              resnet_block_groups=resnet_block_groups,</span><br><span class="line">                              learned_variance=learned_variance,</span><br><span class="line">                              learned_sinusoidal_cond=learned_sinusoidal_cond,</span><br><span class="line">                              random_fourier_features=random_fourier_features,</span><br><span class="line">                              learned_sinusoidal_dim=learned_sinusoidal_dim,</span><br><span class="line">                              condition=condition,</span><br><span class="line">                              input_condition=input_condition)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, time, x_self_cond=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.share_encoder == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.self_condition:</span><br><span class="line">                x_self_cond = default(x_self_cond, <span class="keyword">lambda</span>: torch.zeros_like(x))</span><br><span class="line">                x = torch.cat((x_self_cond, x), dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 如果模型启用了自我条件，则会使用额外的输入条件</span></span><br><span class="line">            <span class="comment"># 如果没有提供，则会默认使用一个与ｘ形状相同的零张量，沿着通道维度进行拼接</span></span><br><span class="line">            x = <span class="variable language_">self</span>.init_conv(x)</span><br><span class="line">            r = x.clone()</span><br><span class="line"></span><br><span class="line">            t = <span class="variable language_">self</span>.time_mlp(time)</span><br><span class="line"></span><br><span class="line">            h = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> block1, block2, attn, downsample <span class="keyword">in</span> <span class="variable language_">self</span>.downs:</span><br><span class="line">                x = block1(x, t)</span><br><span class="line">                h.append(x)</span><br><span class="line"></span><br><span class="line">                x = block2(x, t)</span><br><span class="line">                x = attn(x)</span><br><span class="line">                h.append(x)</span><br><span class="line"></span><br><span class="line">                x = downsample(x)</span><br><span class="line"></span><br><span class="line">            x = <span class="variable language_">self</span>.mid_block1(x, t)</span><br><span class="line">            x = <span class="variable language_">self</span>.mid_attn(x)</span><br><span class="line">            x = <span class="variable language_">self</span>.mid_block2(x, t)</span><br><span class="line"></span><br><span class="line">            out_res = x</span><br><span class="line">            <span class="keyword">for</span> block1, block2, attn, upsample <span class="keyword">in</span> <span class="variable language_">self</span>.ups_no_skip:</span><br><span class="line">                out_res = block1(out_res, t)</span><br><span class="line">                out_res = block2(out_res, t)</span><br><span class="line">                out_res = attn(out_res)</span><br><span class="line"></span><br><span class="line">                out_res = upsample(out_res)</span><br><span class="line">            <span class="comment"># 先经过的是没有跳跃连接的的上采样模块</span></span><br><span class="line">            out_res = <span class="variable language_">self</span>.final_res_block_1(out_res, t)</span><br><span class="line">            out_res = <span class="variable language_">self</span>.final_conv_1(out_res)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> block1, block2, attn, upsample <span class="keyword">in</span> <span class="variable language_">self</span>.ups:</span><br><span class="line">                x = torch.cat((x, h.pop()), dim=<span class="number">1</span>)</span><br><span class="line">                x = block1(x, t)</span><br><span class="line"></span><br><span class="line">                x = torch.cat((x, h.pop()), dim=<span class="number">1</span>)</span><br><span class="line">                x = block2(x, t)</span><br><span class="line">                x = attn(x)</span><br><span class="line"></span><br><span class="line">                x = upsample(x)</span><br><span class="line"></span><br><span class="line">            x = torch.cat((x, r), dim=<span class="number">1</span>)</span><br><span class="line">            x = <span class="variable language_">self</span>.final_res_block_2(x, t)</span><br><span class="line">            out_res_add_noise = <span class="variable language_">self</span>.final_conv_2(x)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> out_res, out_res_add_noise</span><br><span class="line">        <span class="comment"># out_res：通过 self.ups_no_skip 和 final_res_block_1 得到的无噪声的输出结果。</span></span><br><span class="line">    <span class="comment"># out_res_add_noise：通过跳跃连接和 final_res_block_2 得到的输出，可能包含更多的细节或噪声。</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.share_encoder == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.unet0(x, time, x_self_cond=x_self_cond), <span class="variable language_">self</span>.unet1(x, time, x_self_cond=x_self_cond)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.share_encoder == -<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> [<span class="variable language_">self</span>.unet0(x, time, x_self_cond=x_self_cond)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># gaussian diffusion trainer class</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract</span>(<span class="params">a, t, x_shape</span>):</span><br><span class="line">    b, *_ = t.shape</span><br><span class="line">    <span class="comment"># *_表示我们只关心t的第一个维度</span></span><br><span class="line"></span><br><span class="line">    out = a.gather(-<span class="number">1</span>, t)</span><br><span class="line">    <span class="comment"># 表示 a在最后一个维度使用t中的索引来选择值</span></span><br><span class="line">    <span class="keyword">return</span> out.reshape(b, *((<span class="number">1</span>,) * (<span class="built_in">len</span>(x_shape) - <span class="number">1</span>)))</span><br><span class="line"><span class="comment"># 从给定的张量a中提取特定索引t对应的值，并返回一个具有指定形状x_shape的张量</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gen_coefficients</span>(<span class="params">timesteps, schedule=<span class="string">&quot;increased&quot;</span>, sum_scale=<span class="number">1</span></span>):</span><br><span class="line">    <span class="comment"># 用于生成时间步的系数，根据传入的schedule参数，会生成不同的时间步权重（alphas）</span></span><br><span class="line">    <span class="keyword">if</span> schedule == <span class="string">&quot;increased&quot;</span>:</span><br><span class="line">        <span class="comment"># 逐步增加的权重</span></span><br><span class="line">        x = torch.linspace(<span class="number">1</span>, timesteps, timesteps, dtype=torch.float64)</span><br><span class="line">        <span class="comment"># 生成一个1到timesteps的等差数列，包含timesteps个元素</span></span><br><span class="line">        scale = <span class="number">0.5</span>*timesteps*(timesteps+<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 缩放因子</span></span><br><span class="line">        alphas = x/scale</span><br><span class="line">        <span class="comment"># 归一化后的系数列表</span></span><br><span class="line">    <span class="keyword">elif</span> schedule == <span class="string">&quot;decreased&quot;</span>:</span><br><span class="line">        <span class="comment"># # 逐步减少的权重</span></span><br><span class="line">        x = torch.linspace(<span class="number">1</span>, timesteps, timesteps, dtype=torch.float64)</span><br><span class="line">        x = torch.flip(x, dims=[<span class="number">0</span>])</span><br><span class="line">        scale = <span class="number">0.5</span>*timesteps*(timesteps+<span class="number">1</span>)</span><br><span class="line">        alphas = x/scale</span><br><span class="line">    <span class="keyword">elif</span> schedule == <span class="string">&quot;average&quot;</span>:</span><br><span class="line">        <span class="comment"># 均匀的参数</span></span><br><span class="line">        alphas = torch.full([timesteps], <span class="number">1</span>/timesteps, dtype=torch.float64)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        alphas = torch.full([timesteps], <span class="number">1</span>/timesteps, dtype=torch.float64)</span><br><span class="line">    <span class="keyword">assert</span> alphas.<span class="built_in">sum</span>()-torch.tensor(<span class="number">1</span>) &lt; torch.tensor(<span class="number">1e-10</span>)</span><br><span class="line">    <span class="comment"># 确保生成的系数总和非常接近1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> alphas*sum_scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResidualDiffusion</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        model,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        image_size,</span></span><br><span class="line"><span class="params">        timesteps=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">        sampling_timesteps=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        <span class="comment"># 采样时的时间步数.通常在推理时小于训练时的时间步数,用于加速生成过程</span></span></span><br><span class="line"><span class="params">        loss_type=<span class="string">&#x27;l1&#x27;</span>,</span></span><br><span class="line"><span class="params">        objective=<span class="string">&#x27;pred_res_noise&#x27;</span>,</span></span><br><span class="line"><span class="params">        ddim_sampling_eta=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">        <span class="comment"># 控制采样的随机性。如果是 0，表示没有随机性（确定性采样）；如果大于 0，则会引入随机性。</span></span></span><br><span class="line"><span class="params">        condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        sum_scale=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        input_condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        input_condition_mask=<span class="literal">False</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> (</span><br><span class="line">            <span class="built_in">type</span>(<span class="variable language_">self</span>) == ResidualDiffusion <span class="keyword">and</span> model.channels != model.out_dim)</span><br><span class="line">        <span class="keyword">assert</span> <span class="keyword">not</span> model.random_or_learned_sinusoidal_cond</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.model = model</span><br><span class="line">        <span class="variable language_">self</span>.channels = <span class="variable language_">self</span>.model.channels</span><br><span class="line">        <span class="variable language_">self</span>.self_condition = <span class="variable language_">self</span>.model.self_condition</span><br><span class="line">        <span class="variable language_">self</span>.image_size = image_size</span><br><span class="line">        <span class="variable language_">self</span>.objective = objective</span><br><span class="line">        <span class="variable language_">self</span>.condition = condition</span><br><span class="line">        <span class="variable language_">self</span>.input_condition = input_condition</span><br><span class="line">        <span class="variable language_">self</span>.input_condition_mask = input_condition_mask</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            <span class="variable language_">self</span>.sum_scale = sum_scale <span class="keyword">if</span> sum_scale <span class="keyword">else</span> <span class="number">0.01</span></span><br><span class="line">            ddim_sampling_eta = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.sum_scale = sum_scale <span class="keyword">if</span> sum_scale <span class="keyword">else</span> <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">        alphas = gen_coefficients(timesteps, schedule=<span class="string">&quot;decreased&quot;</span>)</span><br><span class="line">        <span class="comment"># 表示图像在扩撒过程中的渐变系数,控制每个时间步的衰减程度</span></span><br><span class="line">        alphas_cumsum = alphas.cumsum(dim=<span class="number">0</span>).clip(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 是alphas的累加和,用于表示每个时间步中,图像的衰减程度</span></span><br><span class="line">        alphas_cumsum_prev = F.pad(alphas_cumsum[:-<span class="number">1</span>], (<span class="number">1</span>, <span class="number">0</span>), value=<span class="number">1.</span>)</span><br><span class="line">        betas2 = gen_coefficients(</span><br><span class="line">            timesteps, schedule=<span class="string">&quot;increased&quot;</span>, sum_scale=<span class="variable language_">self</span>.sum_scale)</span><br><span class="line">        <span class="comment"># 表示每个时间步的噪声大小,控制扩撒过程中的噪声添加</span></span><br><span class="line">        betas2_cumsum = betas2.cumsum(dim=<span class="number">0</span>).clip(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 是 betas2 的累加和，表示噪声的累积效果。</span></span><br><span class="line">        betas_cumsum = torch.sqrt(betas2_cumsum)</span><br><span class="line">        betas2_cumsum_prev = F.pad(betas2_cumsum[:-<span class="number">1</span>], (<span class="number">1</span>, <span class="number">0</span>), value=<span class="number">1.</span>)</span><br><span class="line">        posterior_variance = betas2*betas2_cumsum_prev/betas2_cumsum</span><br><span class="line">        <span class="comment"># 表示每个时间步的后验方差，控制去噪过程中的噪声变化。</span></span><br><span class="line">        <span class="comment"># 扩散模型通常会对输入图像逐步添加噪声（正向过程），然后再通过去噪模型逐步恢复原始图像（反向过程）。</span></span><br><span class="line">        <span class="comment"># 这需要计算后验分布的均值和方差。</span></span><br><span class="line"></span><br><span class="line">        posterior_variance[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        timesteps, = alphas.shape</span><br><span class="line">        <span class="variable language_">self</span>.num_timesteps = <span class="built_in">int</span>(timesteps)</span><br><span class="line">        <span class="variable language_">self</span>.loss_type = loss_type</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sampling related parameters</span></span><br><span class="line">        <span class="comment"># default num sampling timesteps to number of timesteps at training</span></span><br><span class="line">        <span class="variable language_">self</span>.sampling_timesteps = default(sampling_timesteps, timesteps)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="variable language_">self</span>.sampling_timesteps &lt;= timesteps</span><br><span class="line">        <span class="variable language_">self</span>.is_ddim_sampling = <span class="variable language_">self</span>.sampling_timesteps &lt; timesteps</span><br><span class="line">        <span class="comment"># 是否使用DDIM采样方法,通常采样时的步数少于训练时的时间步数</span></span><br><span class="line">        <span class="variable language_">self</span>.ddim_sampling_eta = ddim_sampling_eta</span><br><span class="line">        <span class="comment"># 控制 DDIM 采样的随机性。</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">register_buffer</span>(<span class="params">name, val</span>): <span class="keyword">return</span> <span class="variable language_">self</span>.register_buffer(</span><br><span class="line">            name, val.to(torch.float32))</span><br><span class="line">        <span class="comment"># register_buffer 用于将一个张量注册为模型的缓冲区。</span></span><br><span class="line">        <span class="comment"># 这些张量通常在计算过程中使用，但它们不会被视为模型的参数（即不会更新）。</span></span><br><span class="line">        register_buffer(<span class="string">&#x27;alphas&#x27;</span>, alphas)</span><br><span class="line">        register_buffer(<span class="string">&#x27;alphas_cumsum&#x27;</span>, alphas_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;one_minus_alphas_cumsum&#x27;</span>, <span class="number">1</span>-alphas_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;betas2&#x27;</span>, betas2)</span><br><span class="line">        register_buffer(<span class="string">&#x27;betas&#x27;</span>, torch.sqrt(betas2))</span><br><span class="line">        register_buffer(<span class="string">&#x27;betas2_cumsum&#x27;</span>, betas2_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;betas_cumsum&#x27;</span>, betas_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;posterior_mean_coef1&#x27;</span>,</span><br><span class="line">                        betas2_cumsum_prev/betas2_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;posterior_mean_coef2&#x27;</span>, (betas2 *</span><br><span class="line">                        alphas_cumsum_prev-betas2_cumsum_prev*alphas)/betas2_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;posterior_mean_coef3&#x27;</span>, betas2/betas2_cumsum)</span><br><span class="line">        register_buffer(<span class="string">&#x27;posterior_variance&#x27;</span>, posterior_variance)</span><br><span class="line">        register_buffer(<span class="string">&#x27;posterior_log_variance_clipped&#x27;</span>,</span><br><span class="line">                        torch.log(posterior_variance.clamp(<span class="built_in">min</span>=<span class="number">1e-20</span>)))</span><br><span class="line">        <span class="comment"># 这些缓冲区用于存储扩散过程中的系数和累加值（如 alphas、betas 和它们的累加和），</span></span><br><span class="line">        <span class="comment"># 这些值将在训练和推理过程中被反复使用。</span></span><br><span class="line">        <span class="variable language_">self</span>.posterior_mean_coef1[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.posterior_mean_coef2[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="variable language_">self</span>.posterior_mean_coef3[<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="variable language_">self</span>.one_minus_alphas_cumsum[-<span class="number">1</span>] = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_noise_from_res</span>(<span class="params">self, x_t, t, x_input, pred_res</span>):</span><br><span class="line">    <span class="comment"># 根据模型预测的残差来估计在某个时间步t的噪声，并进行相应的调整</span></span><br><span class="line">    <span class="comment"># x_t是扩散中的某个时间步的图像、x_input输入的原始图像、pred_res模型预测的残差。</span></span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            (x_t-x_input-(extract(<span class="variable language_">self</span>.alphas_cumsum, t, x_t.shape)-<span class="number">1</span>)</span><br><span class="line">             * pred_res)/extract(<span class="variable language_">self</span>.betas_cumsum, t, x_t.shape)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># (extract(self.alphas_cumsum, t, x_t.shape)</span></span><br><span class="line">    <span class="comment"># 用于从 alphas_cumsum 中提取出时间步 t 对应的值，并且将其形状调整为x-t.shape的大小</span></span><br><span class="line">    <span class="comment"># extract 函数的作用是提取每个时间步的系数（alphas_cumsum[t]）并应用于张量 x_t</span></span><br><span class="line">    <span class="comment"># 这是当前图像 x_t 和输入图像 x_input 之间的差异，表示模型预测的噪声部分</span></span><br><span class="line">    <span class="comment"># (extract(self.alphas_cumsum, t, x_t.shape) - 1)：这个操作表明在去噪过程中，alphas_cumsum 的作用是减缓噪声的影响。</span></span><br><span class="line">    <span class="comment"># 将 pred_res（模型预测的残差）与调整系数相乘，得到去噪过程中的噪声预测部分。</span></span><br><span class="line">    <span class="comment"># 最终这一部分表示了噪声的剩余部分。它是通过从当前图像 x_t 中减去输入图像 x_input 以及与残差预测相关的部分来计算的。</span></span><br><span class="line">    <span class="comment"># extract(self.betas_cumsum, t, x_t.shape) 提取当前时间步 t 对应的噪声累积值，并将其形状调整为 x_t.shape。</span></span><br><span class="line">    <span class="comment"># 这一步用来对计算得到的噪声进行尺度变换</span></span><br><span class="line">    <span class="comment"># =========================================&gt; 总结 &lt;===========================================</span></span><br><span class="line">    <span class="comment"># 计算出当前图像 x_t 和输入图像 x_input 之间的差异。</span></span><br><span class="line">    <span class="comment"># 根据模型预测的残差 pred_res，调整这一差异。</span></span><br><span class="line">    <span class="comment"># 使用 alphas_cumsum 和 betas_cumsum 对结果进行标准化，得到最终的噪声预测。</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_start_from_xinput_noise</span>(<span class="params">self, x_t, t, x_input, noise</span>):</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            (x_t-extract(<span class="variable language_">self</span>.alphas_cumsum, t, x_t.shape)*x_input -</span><br><span class="line">             extract(<span class="variable language_">self</span>.betas_cumsum, t, x_t.shape) * noise)/extract(<span class="variable language_">self</span>.one_minus_alphas_cumsum, t, x_t.shape)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 这个函数的目的是从时间步t上的噪声图像x_t中，利用已知的扩散模型参数预测去噪后的图像</span></span><br><span class="line">    <span class="comment"># 这个函数的目的是从时间步 t 上的噪声图像 x_t 中，利用已知的扩散模型参数（如 alphas_cumsum, betas_cumsum）</span></span><br><span class="line">    <span class="comment"># 和额外的输入 x_input，预测图像的“起始”状态（即去噪后的图像）。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_start_from_res_noise</span>(<span class="params">self, x_t, t, x_res, noise</span>):</span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            x_t-extract(<span class="variable language_">self</span>.alphas_cumsum, t, x_t.shape) * x_res -</span><br><span class="line">            extract(<span class="variable language_">self</span>.betas_cumsum, t, x_t.shape) * noise</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 用来从当前噪声图像 x_t 和残差图像 x_res 以及噪声 noise 中，推断出清晰图像或上一时间步的图像。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">q_posterior_from_res_noise</span>(<span class="params">self, x_res, noise, x_t, t</span>):</span><br><span class="line">        <span class="keyword">return</span> (x_t-extract(<span class="variable language_">self</span>.alphas, t, x_t.shape) * x_res -</span><br><span class="line">                (extract(<span class="variable language_">self</span>.betas2, t, x_t.shape)/extract(<span class="variable language_">self</span>.betas_cumsum, t, x_t.shape)) * noise)</span><br><span class="line">    <span class="comment"># 通过给定的残差图像、噪声、和当前噪声图像计算当前时间步的后验分布</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">q_posterior</span>(<span class="params">self, pred_res, x_start, x_t, t</span>):</span><br><span class="line">        posterior_mean = (</span><br><span class="line">            extract(<span class="variable language_">self</span>.posterior_mean_coef1, t, x_t.shape) * x_t +</span><br><span class="line">            extract(<span class="variable language_">self</span>.posterior_mean_coef2, t, x_t.shape) * pred_res +</span><br><span class="line">            extract(<span class="variable language_">self</span>.posterior_mean_coef3, t, x_t.shape) * x_start</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 后验分布的均值</span></span><br><span class="line">        posterior_variance = extract(<span class="variable language_">self</span>.posterior_variance, t, x_t.shape)</span><br><span class="line">        <span class="comment"># 后验分布的方差</span></span><br><span class="line">        posterior_log_variance_clipped = extract(</span><br><span class="line">            <span class="variable language_">self</span>.posterior_log_variance_clipped, t, x_t.shape)</span><br><span class="line">        <span class="comment"># 裁剪后的后验对数方差</span></span><br><span class="line">        <span class="keyword">return</span> posterior_mean, posterior_variance, posterior_log_variance_clipped</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">model_predictions</span>(<span class="params">self, x_input, x, t, x_input_condition=<span class="number">0</span>, x_self_cond=<span class="literal">None</span>, clip_denoised=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            x_in = x</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition:</span><br><span class="line">                x_in = torch.cat((x, x_input, x_input_condition), dim=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x_in = torch.cat((x, x_input), dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 根据条件来选择合适的输入形式</span></span><br><span class="line">        model_output = <span class="variable language_">self</span>.model(x_in,</span><br><span class="line">                                  t,</span><br><span class="line">                                  x_self_cond)</span><br><span class="line">        maybe_clip = partial(torch.clamp, <span class="built_in">min</span>=-<span class="number">1.</span>,</span><br><span class="line">                             <span class="built_in">max</span>=<span class="number">1.</span>) <span class="keyword">if</span> clip_denoised <span class="keyword">else</span> identity</span><br><span class="line">        <span class="comment"># 定义一个剪裁去噪效果</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_res_noise&#x27;</span>:</span><br><span class="line">            pred_res = model_output[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_output[<span class="number">1</span>]</span><br><span class="line">            pred_res = maybe_clip(pred_res)</span><br><span class="line">            x_start = <span class="variable language_">self</span>.predict_start_from_res_noise(</span><br><span class="line">                x, t, pred_res, pred_noise)</span><br><span class="line">            x_start = maybe_clip(x_start)</span><br><span class="line">            <span class="comment"># 预测最后的清晰起始图像</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_res_add_noise&#x27;</span>:</span><br><span class="line">            pred_res = model_output[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_output[<span class="number">1</span>] - model_output[<span class="number">0</span>]</span><br><span class="line">            pred_res = maybe_clip(pred_res)</span><br><span class="line">            x_start = <span class="variable language_">self</span>.predict_start_from_res_noise(</span><br><span class="line">                x, t, pred_res, pred_noise)</span><br><span class="line">            x_start = maybe_clip(x_start)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_x0_noise&#x27;</span>:</span><br><span class="line">            pred_res = x_input-model_output[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_output[<span class="number">1</span>]</span><br><span class="line">            pred_res = maybe_clip(pred_res)</span><br><span class="line">            x_start = maybe_clip(model_output[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_x0_add_noise&#x27;</span>:</span><br><span class="line">            x_start = model_output[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_output[<span class="number">1</span>] - model_output[<span class="number">0</span>]</span><br><span class="line">            pred_res = x_input-x_start</span><br><span class="line">            pred_res = maybe_clip(pred_res)</span><br><span class="line">            x_start = maybe_clip(model_output[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&quot;pred_noise&quot;</span>:</span><br><span class="line">            pred_noise = model_output[<span class="number">0</span>]</span><br><span class="line">            x_start = <span class="variable language_">self</span>.predict_start_from_xinput_noise(</span><br><span class="line">                x, t, x_input, pred_noise)</span><br><span class="line">            x_start = maybe_clip(x_start)</span><br><span class="line">            pred_res = x_input - x_start</span><br><span class="line">            pred_res = maybe_clip(pred_res)</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&quot;pred_res&quot;</span>:</span><br><span class="line">            pred_res = model_output[<span class="number">0</span>]</span><br><span class="line">            pred_res = maybe_clip(pred_res)</span><br><span class="line">            pred_noise = <span class="variable language_">self</span>.predict_noise_from_res(x, t, x_input, pred_res)</span><br><span class="line">            x_start = x_input - pred_res</span><br><span class="line">            x_start = maybe_clip(x_start)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ModelResPrediction(pred_res, pred_noise, x_start)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">p_mean_variance</span>(<span class="params">self, x_input, x, t, x_input_condition=<span class="number">0</span>, x_self_cond=<span class="literal">None</span></span>):</span><br><span class="line">        preds = <span class="variable language_">self</span>.model_predictions(</span><br><span class="line">            x_input, x, t, x_input_condition, x_self_cond)</span><br><span class="line">        pred_res = preds.pred_res</span><br><span class="line">        x_start = preds.pred_x_start</span><br><span class="line"></span><br><span class="line">        model_mean, posterior_variance, posterior_log_variance = <span class="variable language_">self</span>.q_posterior(</span><br><span class="line">            pred_res=pred_res, x_start=x_start, x_t=x, t=t)</span><br><span class="line">        <span class="comment"># 表示模型预测的当前时间步图像的均值,以及后验分布的方差,后验分布的对数方差</span></span><br><span class="line">        <span class="keyword">return</span> model_mean, posterior_variance, posterior_log_variance, x_start</span><br><span class="line">    <span class="comment"># 目的是计算给定当前噪声图像x,时间步t,条件输入x_input等信息时的均值和方差.</span></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="comment"># 因为在推理阶段我们不需要计算梯度.所以使用这个装饰器可以减少内存使用并加速推理过程</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">p_sample</span>(<span class="params">self, x_input, x, t: <span class="built_in">int</span>, x_input_condition=<span class="number">0</span>, x_self_cond=<span class="literal">None</span></span>):</span><br><span class="line">        b, *_, device = *x.shape, x.device</span><br><span class="line">        batched_times = torch.full(</span><br><span class="line">            (x.shape[<span class="number">0</span>],), t, device=x.device, dtype=torch.long)</span><br><span class="line">        model_mean, _, model_log_variance, x_start = <span class="variable language_">self</span>.p_mean_variance(</span><br><span class="line">            x_input, x=x, t=batched_times, x_input_condition=x_input_condition, x_self_cond=x_self_cond)</span><br><span class="line">        noise = torch.randn_like(x) <span class="keyword">if</span> t &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0.</span>  <span class="comment"># no noise if t == 0</span></span><br><span class="line">        pred_img = model_mean + (<span class="number">0.5</span> * model_log_variance).exp() * noise</span><br><span class="line">        <span class="keyword">return</span> pred_img, x_start</span><br><span class="line"><span class="comment"># p_sample用于生成模型的样本,给定时间步t和当前噪声图像x的情况下,生成一个预测的图像</span></span><br><span class="line"><span class="comment"># pred_img 是模型预测的当前时间步的图像，它是均值加上缩放后的噪声。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">p_sample_loop</span>(<span class="params">self, x_input, shape, last=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition:</span><br><span class="line">            x_input_condition = x_input[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x_input_condition = <span class="number">0</span></span><br><span class="line">        x_input = x_input[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        batch, device = shape[<span class="number">0</span>], <span class="variable language_">self</span>.betas.device</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            img = x_input+math.sqrt(<span class="variable language_">self</span>.sum_scale) * \</span><br><span class="line">                torch.randn(shape, device=device)</span><br><span class="line">            input_add_noise = img</span><br><span class="line">        <span class="comment"># 如果self.condition为true,则生成带噪声的图像img</span></span><br><span class="line">        <span class="comment"># x_input是输入图像,math.sqrt(self.sum_scale)是缩放因子,</span></span><br><span class="line">        <span class="comment"># torch.randn(shape, device=device) 生成与目标图像形状相同的随机噪声，并将其加到 x_input 上。</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = torch.randn(shape, device=device)</span><br><span class="line"></span><br><span class="line">        x_start = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 在此函数中初始化为None,它将在循环中逐步更新</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">            img_list = []</span><br><span class="line">        <span class="comment"># 用于存储每个时间步的生成图像</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> tqdm(<span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps)), desc=<span class="string">&#x27;sampling loop time step&#x27;</span>, total=<span class="variable language_">self</span>.num_timesteps):</span><br><span class="line">            self_cond = x_start <span class="keyword">if</span> <span class="variable language_">self</span>.self_condition <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            <span class="comment"># self_cond 只有在 self.self_condition 为 True 时才会传递给 p_sample，否则为 None。</span></span><br><span class="line">            img, x_start = <span class="variable language_">self</span>.p_sample(</span><br><span class="line">                x_input, img, t, x_input_condition, self_cond)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                img_list.append(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                img_list = [input_add_noise]+img_list</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                img_list = [input_add_noise, img]</span><br><span class="line">            <span class="keyword">return</span> unnormalize_to_zero_to_one(img_list)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                img_list = img_list</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                img_list = [img]</span><br><span class="line">            <span class="keyword">return</span> unnormalize_to_zero_to_one(img_list)</span><br><span class="line"><span class="comment">#  p_sample_loop是一个采样循环函数,用于从噪声中逐步生成图像,是一个更高层的函数</span></span><br><span class="line"><span class="comment"># 通过多次调用p_sample将噪声图像反向扩散回一个清晰图像</span></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">ddim_sample</span>(<span class="params">self, x_input, shape, last=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition:</span><br><span class="line">            x_input_condition = x_input[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x_input_condition = <span class="number">0</span></span><br><span class="line">        x_input = x_input[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[</span><br><span class="line">            <span class="number">0</span>], <span class="variable language_">self</span>.betas.device, <span class="variable language_">self</span>.num_timesteps, <span class="variable language_">self</span>.sampling_timesteps, <span class="variable language_">self</span>.ddim_sampling_eta, <span class="variable language_">self</span>.objective</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps</span></span><br><span class="line">        times = torch.linspace(-<span class="number">1</span>, total_timesteps - <span class="number">1</span>,</span><br><span class="line">                               steps=sampling_timesteps + <span class="number">1</span>)</span><br><span class="line">        times = <span class="built_in">list</span>(<span class="built_in">reversed</span>(times.<span class="built_in">int</span>().tolist()))</span><br><span class="line">        <span class="comment"># [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]</span></span><br><span class="line">        time_pairs = <span class="built_in">list</span>(<span class="built_in">zip</span>(times[:-<span class="number">1</span>], times[<span class="number">1</span>:]))</span><br><span class="line">        <span class="comment"># 生成时间步对</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            img = x_input+math.sqrt(<span class="variable language_">self</span>.sum_scale) * \</span><br><span class="line">                torch.randn(shape, device=device)</span><br><span class="line">            input_add_noise = img</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = torch.randn(shape, device=device)</span><br><span class="line"></span><br><span class="line">        x_start = <span class="literal">None</span></span><br><span class="line">        <span class="built_in">type</span> = <span class="string">&quot;use_pred_noise&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">            img_list = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> time, time_next <span class="keyword">in</span> tqdm(time_pairs, desc=<span class="string">&#x27;sampling loop time step&#x27;</span>):</span><br><span class="line">            <span class="comment"># 从time_pairs中获取每对时间对,并依次对每个时间步进行采样</span></span><br><span class="line">            time_cond = torch.full(</span><br><span class="line">                (batch,), time, device=device, dtype=torch.long)</span><br><span class="line">            <span class="comment"># 当前时间步time对应的张量</span></span><br><span class="line">            self_cond = x_start <span class="keyword">if</span> <span class="variable language_">self</span>.self_condition <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            preds = <span class="variable language_">self</span>.model_predictions(</span><br><span class="line">                x_input, img, time_cond, x_input_condition, self_cond)</span><br><span class="line"></span><br><span class="line">            pred_res = preds.pred_res</span><br><span class="line">            pred_noise = preds.pred_noise</span><br><span class="line">            x_start = preds.pred_x_start</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> time_next &lt; <span class="number">0</span>:</span><br><span class="line">                img = x_start</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                    img_list.append(img)</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 如果时间步小于0.最后一步,直接将x_start作为图像</span></span><br><span class="line">            alpha_cumsum = <span class="variable language_">self</span>.alphas_cumsum[time]</span><br><span class="line">            alpha_cumsum_next = <span class="variable language_">self</span>.alphas_cumsum[time_next]</span><br><span class="line">            alpha = alpha_cumsum-alpha_cumsum_next</span><br><span class="line"></span><br><span class="line">            betas2_cumsum = <span class="variable language_">self</span>.betas2_cumsum[time]</span><br><span class="line">            betas2_cumsum_next = <span class="variable language_">self</span>.betas2_cumsum[time_next]</span><br><span class="line">            betas2 = betas2_cumsum-betas2_cumsum_next</span><br><span class="line">            betas = betas2.sqrt()</span><br><span class="line">            betas_cumsum = <span class="variable language_">self</span>.betas_cumsum[time]</span><br><span class="line">            betas_cumsum_next = <span class="variable language_">self</span>.betas_cumsum[time_next]</span><br><span class="line">            <span class="comment"># 计算在当前时间步和下一个时间步的累积系数（alpha_cumsum, betas_cumsum）和差异。</span></span><br><span class="line">            sigma2 = eta * (betas2*betas2_cumsum_next/betas2_cumsum)</span><br><span class="line">            sqrt_betas2_cumsum_next_minus_sigma2_divided_betas_cumsum = (</span><br><span class="line">                betas2_cumsum_next-sigma2).sqrt()/betas_cumsum</span><br><span class="line">            <span class="comment"># 计算噪声标准差（sigma2）并用于调整噪声的强度。</span></span><br><span class="line">            <span class="keyword">if</span> eta == <span class="number">0</span>:</span><br><span class="line">                noise = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                noise = torch.randn_like(img)</span><br><span class="line">            <span class="comment"># 根据 type 选择不同的采样策略，</span></span><br><span class="line">            <span class="comment"># 如 &quot;use_pred_noise&quot;、&quot;use_x_start&quot;、&quot;special_eta_0&quot; 等。</span></span><br><span class="line">            <span class="comment"># 每种策略使用不同的公式来更新图像。</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">type</span> == <span class="string">&quot;use_pred_noise&quot;</span>:</span><br><span class="line">                img = img - alpha*pred_res - \</span><br><span class="line">                    (betas_cumsum-(betas2_cumsum_next-sigma2).sqrt()) * \</span><br><span class="line">                    pred_noise + sigma2.sqrt()*noise</span><br><span class="line">            <span class="comment"># alpha是一个权重因子,与时间步相关,表示在当前时间步对图像去噪的强度</span></span><br><span class="line">            <span class="comment"># pred_res是模型的一簇额残差,用来从当前图像中恢复清晰图像的一部分</span></span><br><span class="line">            <span class="comment"># sigma2.sqrt()*noise控制了噪声的强度,sigma2是一个与时间步相关的参数,决定了噪声的尺度</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&quot;use_x_start&quot;</span>:</span><br><span class="line">                img = sqrt_betas2_cumsum_next_minus_sigma2_divided_betas_cumsum*img + \</span><br><span class="line">                    (<span class="number">1</span>-sqrt_betas2_cumsum_next_minus_sigma2_divided_betas_cumsum)*x_start + \</span><br><span class="line">                    (alpha_cumsum_next-alpha_cumsum*sqrt_betas2_cumsum_next_minus_sigma2_divided_betas_cumsum)*pred_res + \</span><br><span class="line">                    sigma2.sqrt()*noise</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&quot;special_eta_0&quot;</span>:</span><br><span class="line">                img = img - alpha*pred_res - \</span><br><span class="line">                    (betas_cumsum-betas_cumsum_next)*pred_noise</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">type</span> == <span class="string">&quot;special_eta_1&quot;</span>:</span><br><span class="line">                img = img - alpha*pred_res - betas2/betas_cumsum*pred_noise + \</span><br><span class="line">                    betas*betas2_cumsum_next.sqrt()/betas_cumsum*noise</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                img_list.append(img)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                img_list = [input_add_noise]+img_list</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                img_list = [input_add_noise, img]</span><br><span class="line">            <span class="keyword">return</span> unnormalize_to_zero_to_one(img_list)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> last:</span><br><span class="line">                img_list = img_list</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                img_list = [img]</span><br><span class="line">            <span class="keyword">return</span> unnormalize_to_zero_to_one(img_list)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, x_input=<span class="number">0</span>, batch_size=<span class="number">16</span>, last=<span class="literal">True</span></span>):</span><br><span class="line">        image_size, channels = <span class="variable language_">self</span>.image_size, <span class="variable language_">self</span>.channels</span><br><span class="line">        sample_fn = <span class="variable language_">self</span>.p_sample_loop <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.is_ddim_sampling <span class="keyword">else</span> <span class="variable language_">self</span>.ddim_sample</span><br><span class="line">        <span class="comment"># 选择了标准的扩散采样方法</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition <span class="keyword">and</span> <span class="variable language_">self</span>.input_condition_mask:</span><br><span class="line">                x_input[<span class="number">0</span>] = normalize_to_neg_one_to_one(x_input[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x_input = normalize_to_neg_one_to_one(x_input)</span><br><span class="line">            batch_size, channels, h, w = x_input[<span class="number">0</span>].shape</span><br><span class="line">            size = (batch_size, channels, h, w)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            size = (batch_size, channels, image_size, image_size)</span><br><span class="line">        <span class="keyword">return</span> sample_fn(x_input, size, last=last)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">q_sample</span>(<span class="params">self, x_start, x_res, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">        noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            x_start+extract(<span class="variable language_">self</span>.alphas_cumsum, t, x_start.shape) * x_res +</span><br><span class="line">            extract(<span class="variable language_">self</span>.betas_cumsum, t, x_start.shape) * noise</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># q_Sample是通过加权残差和噪声来模拟扩散过程的前向传播</span></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="comment"># 将一个方法转化为属性访问</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">loss_fn</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.loss_type == <span class="string">&#x27;l1&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> F.l1_loss</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.loss_type == <span class="string">&#x27;l2&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> F.mse_loss</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&#x27;invalid loss type <span class="subst">&#123;self.loss_type&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">p_losses</span>(<span class="params">self, imgs, t, noise=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 计算训练过程中计算损失的函数</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(imgs, <span class="built_in">list</span>):  <span class="comment"># Condition</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition:</span><br><span class="line">                x_input_condition = imgs[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x_input_condition = <span class="number">0</span></span><br><span class="line">            x_input = imgs[<span class="number">1</span>]</span><br><span class="line">            x_start = imgs[<span class="number">0</span>]  <span class="comment"># gt = imgs[0], input = imgs[1]</span></span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># Generation</span></span><br><span class="line">            x_input = <span class="number">0</span></span><br><span class="line">            x_start = imgs</span><br><span class="line"></span><br><span class="line">        noise = default(noise, <span class="keyword">lambda</span>: torch.randn_like(x_start))</span><br><span class="line">        x_res = x_input - x_start</span><br><span class="line"></span><br><span class="line">        b, c, h, w = x_start.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># noise sample</span></span><br><span class="line">        x = <span class="variable language_">self</span>.q_sample(x_start, x_res, t, noise=noise)</span><br><span class="line">        <span class="comment"># 扩散中的一个步骤,负责生成一个具有噪声的样本</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># if doing self-conditioning, 50% of the time, predict x_start from current set of times</span></span><br><span class="line">        <span class="comment"># and condition with unet with that</span></span><br><span class="line">        <span class="comment"># this technique will slow down training by 25%, but seems to lower FID significantly</span></span><br><span class="line">        x_self_cond = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.self_condition <span class="keyword">and</span> random.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                x_self_cond = <span class="variable language_">self</span>.model_predictions(</span><br><span class="line">                    x_input, x, t, x_input_condition <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition <span class="keyword">else</span> <span class="number">0</span>).pred_x_start</span><br><span class="line">                x_self_cond.detach_()</span><br><span class="line">        <span class="comment"># 自我条件是一个训练技巧,用于通过模型的预测来引导模型的学习过程.</span></span><br><span class="line">        <span class="comment"># 每50%的时间步模型会尝试用自我条件化技术来生成一个新的预测图像,并将其用于当前的步骤.</span></span><br><span class="line">        <span class="comment"># x_self_cond.detach_()表示不计算梯度，即该图像不会参与反向传播</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># predict and take gradient step</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            x_in = x</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition:</span><br><span class="line">                x_in = torch.cat((x, x_input, x_input_condition), dim=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x_in = torch.cat((x, x_input), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        model_out = <span class="variable language_">self</span>.model(x_in,</span><br><span class="line">                               t,</span><br><span class="line">                               x_self_cond)</span><br><span class="line"></span><br><span class="line">        target = []</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_res_noise&#x27;</span>:</span><br><span class="line">            <span class="comment"># 目标是残差和噪声</span></span><br><span class="line">            target.append(x_res)</span><br><span class="line">            target.append(noise)</span><br><span class="line"></span><br><span class="line">            pred_res = model_out[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_out[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_res_add_noise&#x27;</span>:</span><br><span class="line">            target.append(x_res)</span><br><span class="line">            target.append(x_res+noise)</span><br><span class="line"></span><br><span class="line">            pred_res = model_out[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_out[<span class="number">1</span>]-model_out[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_x0_noise&#x27;</span>:</span><br><span class="line">            target.append(x_start)</span><br><span class="line">            target.append(noise)</span><br><span class="line"></span><br><span class="line">            pred_res = x_input-model_out[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_out[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&#x27;pred_x0_add_noise&#x27;</span>:</span><br><span class="line">            target.append(x_start)</span><br><span class="line">            target.append(x_start+noise)</span><br><span class="line"></span><br><span class="line">            pred_res = x_input-model_out[<span class="number">0</span>]</span><br><span class="line">            pred_noise = model_out[<span class="number">1</span>] - model_out[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&quot;pred_noise&quot;</span>:</span><br><span class="line">            target.append(noise)</span><br><span class="line"></span><br><span class="line">            pred_noise = model_out[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.objective == <span class="string">&quot;pred_res&quot;</span>:</span><br><span class="line">            target.append(x_res)</span><br><span class="line"></span><br><span class="line">            pred_res = model_out[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">f&#x27;unknown objective <span class="subst">&#123;self.objective&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        u_loss = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> u_loss:</span><br><span class="line">            x_u = <span class="variable language_">self</span>.q_posterior_from_res_noise(pred_res, pred_noise, x, t)</span><br><span class="line">            u_gt = <span class="variable language_">self</span>.q_posterior_from_res_noise(x_res, noise, x, t)</span><br><span class="line">            loss = <span class="number">10000</span>*<span class="variable language_">self</span>.loss_fn(x_u, u_gt, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">            <span class="comment"># 如果设置为true，则会执行一种特殊的损失计算方式。</span></span><br><span class="line">            <span class="comment"># 使用q_posterior_from_res_noise计算后验分布的损失</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(model_out)):</span><br><span class="line">                loss = loss + \</span><br><span class="line">                    <span class="variable language_">self</span>.loss_fn(model_out[i], target[i], reduction=<span class="string">&#x27;none&#x27;</span>)</span><br><span class="line">        loss = reduce(loss, <span class="string">&#x27;b ... -&gt; b (...)&#x27;</span>, <span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        <span class="comment"># 否则，标准损失计算：对于每个模型输出，计算与目标的损失，并对所有的损失进行累加。</span></span><br><span class="line">        <span class="comment"># 使用 reduce 函数将损失求平均，并返回最终的损失值。</span></span><br><span class="line">        <span class="keyword">return</span> loss.mean()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, img, *args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(img, <span class="built_in">list</span>):</span><br><span class="line">            <span class="comment"># 判断img是不是一个列表</span></span><br><span class="line">            b, c, h, w, device, img_size, = * \</span><br><span class="line">                img[<span class="number">0</span>].shape, img[<span class="number">0</span>].device, <span class="variable language_">self</span>.image_size</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            b, c, h, w, device, img_size, = *img.shape, img.device, <span class="variable language_">self</span>.image_size</span><br><span class="line">        <span class="comment"># assert h == img_size and w == img_size, f&#x27;height and width of image must be &#123;img_size&#125;&#x27;</span></span><br><span class="line">        t = torch.randint(<span class="number">0</span>, <span class="variable language_">self</span>.num_timesteps, (b,), device=device).long()</span><br><span class="line">        <span class="comment"># 生一个形状为（b,）的随机整数张量t</span></span><br><span class="line">        <span class="comment"># 每个元素表示一个随机选定的时间步，（0到self.num_timesteps - 1 之间）</span></span><br><span class="line">        <span class="comment"># 这些时间步用于扩散模型的训练或推理阶段</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.input_condition <span class="keyword">and</span> <span class="variable language_">self</span>.input_condition_mask:</span><br><span class="line">            img[<span class="number">0</span>] = normalize_to_neg_one_to_one(img[<span class="number">0</span>])</span><br><span class="line">            img[<span class="number">1</span>] = normalize_to_neg_one_to_one(img[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            img = normalize_to_neg_one_to_one(img)</span><br><span class="line">        <span class="comment"># 将图片归一化到[-1,1]</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.p_losses(img, t, *args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># trainer class</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Trainer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        diffusion_model,</span></span><br><span class="line"><span class="params">        folder,</span></span><br><span class="line"><span class="params">        *,</span></span><br><span class="line"><span class="params">        train_batch_size=<span class="number">16</span>,</span></span><br><span class="line"><span class="params">        gradient_accumulate_every=<span class="number">1</span>,<span class="comment"># 每个批次更新一次梯度</span></span></span><br><span class="line"><span class="params">        augment_flip=<span class="literal">True</span>,<span class="comment"># 是否进行数据增强</span></span></span><br><span class="line"><span class="params">        train_lr=<span class="number">1e-4</span>,</span></span><br><span class="line"><span class="params">        train_num_steps=<span class="number">100000</span>,</span></span><br><span class="line"><span class="params">        ema_update_every=<span class="number">10</span>,</span></span><br><span class="line"><span class="params">        ema_decay=<span class="number">0.995</span>,<span class="comment"># EMA 更新的衰减系数，默认为 0.995</span></span></span><br><span class="line"><span class="params">        adam_betas=(<span class="params"><span class="number">0.9</span>, <span class="number">0.99</span></span>),<span class="comment"># Adam 优化器的 β 参数，默认为 (0.9, 0.99)</span></span></span><br><span class="line"><span class="params">        save_and_sample_every=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">        num_samples=<span class="number">25</span>,<span class="comment"># 每次保存时生成的样本数，默认为 25</span></span></span><br><span class="line"><span class="params">        results_folder=<span class="string">&#x27;./results/sample&#x27;</span>,</span></span><br><span class="line"><span class="params">        amp=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        fp16=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        split_batches=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">        convert_image_to=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">        condition=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        sub_dir=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        equalizeHist=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        crop_patch=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">        generation=<span class="literal">False</span></span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.accelerator = Accelerator(</span><br><span class="line">            split_batches=split_batches,</span><br><span class="line">            mixed_precision=<span class="string">&#x27;fp16&#x27;</span> <span class="keyword">if</span> fp16 <span class="keyword">else</span> <span class="string">&#x27;no&#x27;</span></span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 它可以帮助在多设备（如多 GPU）上分布式训练，并支持自动混合精度（AMP）。</span></span><br><span class="line">        <span class="comment"># 根据 fp16 的设置，它会启用或禁用混合精度训练。</span></span><br><span class="line">        <span class="variable language_">self</span>.sub_dir = sub_dir</span><br><span class="line">        <span class="variable language_">self</span>.crop_patch = crop_patch</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.accelerator.native_amp = amp</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.model = diffusion_model</span><br><span class="line">        <span class="comment"># self.model 将传入的扩散模型对象赋值给 self.model，用于后续训练。</span></span><br><span class="line">        <span class="keyword">assert</span> has_int_squareroot(</span><br><span class="line">            num_samples), <span class="string">&#x27;number of samples must have an integer square root&#x27;</span></span><br><span class="line">        <span class="comment"># 判断是不是一个完全平方数</span></span><br><span class="line">        <span class="comment"># 这通常用于生成正方形的图像（例如 25 张样本的形状是 5x5）</span></span><br><span class="line">        <span class="variable language_">self</span>.num_samples = num_samples</span><br><span class="line">        <span class="variable language_">self</span>.save_and_sample_every = save_and_sample_every</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.batch_size = train_batch_size</span><br><span class="line">        <span class="variable language_">self</span>.gradient_accumulate_every = gradient_accumulate_every</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.train_num_steps = train_num_steps</span><br><span class="line">        <span class="variable language_">self</span>.image_size = diffusion_model.image_size</span><br><span class="line">        <span class="variable language_">self</span>.condition = condition</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(folder) == <span class="number">3</span>:</span><br><span class="line">                <span class="variable language_">self</span>.condition_type = <span class="number">1</span></span><br><span class="line">                <span class="comment"># test_input</span></span><br><span class="line">                ds = dataset(folder[-<span class="number">1</span>], <span class="variable language_">self</span>.image_size,</span><br><span class="line">                             augment_flip=<span class="literal">False</span>, convert_image_to=convert_image_to, condition=<span class="number">0</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, sample=<span class="literal">True</span>, generation=generation)</span><br><span class="line">                trian_folder = folder[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.sample_dataset = ds</span><br><span class="line">                <span class="variable language_">self</span>.sample_loader = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(<span class="variable language_">self</span>.sample_dataset, batch_size=num_samples, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                                               pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))  <span class="comment"># cpu_count()</span></span><br><span class="line"></span><br><span class="line">                ds = dataset(trian_folder, <span class="variable language_">self</span>.image_size, augment_flip=augment_flip,</span><br><span class="line">                             convert_image_to=convert_image_to, condition=<span class="number">1</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, generation=generation)</span><br><span class="line">                <span class="variable language_">self</span>.dl = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(ds, batch_size=train_batch_size,</span><br><span class="line">                                shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">len</span>(folder) == <span class="number">4</span>:</span><br><span class="line">                <span class="variable language_">self</span>.condition_type = <span class="number">2</span></span><br><span class="line">                <span class="comment"># test_gt+test_input</span></span><br><span class="line">                ds = dataset(folder[<span class="number">2</span>:<span class="number">4</span>], <span class="variable language_">self</span>.image_size,</span><br><span class="line">                             augment_flip=<span class="literal">False</span>, convert_image_to=convert_image_to, condition=<span class="number">1</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, sample=<span class="literal">True</span>, generation=generation)</span><br><span class="line">                trian_folder = folder[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.sample_dataset = ds</span><br><span class="line">                <span class="variable language_">self</span>.sample_loader = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(<span class="variable language_">self</span>.sample_dataset, batch_size=num_samples, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                                               pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))  <span class="comment"># cpu_count()</span></span><br><span class="line"></span><br><span class="line">                ds = dataset(trian_folder, <span class="variable language_">self</span>.image_size, augment_flip=augment_flip,</span><br><span class="line">                             convert_image_to=convert_image_to, condition=<span class="number">1</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, generation=generation)</span><br><span class="line">                <span class="variable language_">self</span>.dl = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(ds, batch_size=train_batch_size,</span><br><span class="line">                                shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))</span><br><span class="line">            <span class="keyword">elif</span> <span class="built_in">len</span>(folder) == <span class="number">6</span>:</span><br><span class="line">                <span class="variable language_">self</span>.condition_type = <span class="number">3</span></span><br><span class="line">                <span class="comment"># test_gt+test_input</span></span><br><span class="line">                ds = dataset(folder[<span class="number">3</span>:<span class="number">6</span>], <span class="variable language_">self</span>.image_size,</span><br><span class="line">                             augment_flip=<span class="literal">False</span>, convert_image_to=convert_image_to, condition=<span class="number">2</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, sample=<span class="literal">True</span>, generation=generation)</span><br><span class="line">                trian_folder = folder[<span class="number">0</span>:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.sample_dataset = ds</span><br><span class="line">                <span class="variable language_">self</span>.sample_loader = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(<span class="variable language_">self</span>.sample_dataset, batch_size=num_samples, shuffle=<span class="literal">True</span>,</span><br><span class="line">                                                                               pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))  <span class="comment"># cpu_count()</span></span><br><span class="line"></span><br><span class="line">                ds = dataset(trian_folder, <span class="variable language_">self</span>.image_size, augment_flip=augment_flip,</span><br><span class="line">                             convert_image_to=convert_image_to, condition=<span class="number">2</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, generation=generation)</span><br><span class="line">                <span class="variable language_">self</span>.dl = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(ds, batch_size=train_batch_size,</span><br><span class="line">                                shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="variable language_">self</span>.condition_type = <span class="number">0</span></span><br><span class="line">            trian_folder = folder</span><br><span class="line"></span><br><span class="line">            ds = dataset(trian_folder, <span class="variable language_">self</span>.image_size, augment_flip=augment_flip,</span><br><span class="line">                         convert_image_to=convert_image_to, condition=<span class="number">0</span>, equalizeHist=equalizeHist, crop_patch=crop_patch, generation=generation)</span><br><span class="line">            <span class="variable language_">self</span>.dl = cycle(<span class="variable language_">self</span>.accelerator.prepare(DataLoader(ds, batch_size=train_batch_size,</span><br><span class="line">                            shuffle=<span class="literal">True</span>, pin_memory=<span class="literal">True</span>, num_workers=<span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># optimizer</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.opt = Adam(diffusion_model.parameters(),</span><br><span class="line">                        lr=train_lr, betas=adam_betas)</span><br><span class="line"><span class="comment"># self.opt = Adam(...)：使用 Adam 优化器来更新模型的参数。</span></span><br><span class="line"><span class="comment"># diffusion_model.parameters()：</span></span><br><span class="line"><span class="comment"># 获取 diffusion_model（扩散模型）中所有需要优化的参数。</span></span><br><span class="line"><span class="comment"># lr=train_lr：学习率 (train_lr) 用于设置优化器的学习速率。</span></span><br><span class="line"><span class="comment"># betas=adam_betas：betas 是 Adam 优化器的两个超参数，通常分别是 (β1, β2)，</span></span><br><span class="line"><span class="comment"># 它们控制一阶矩估计和二阶矩估计的衰减率。adam_betas 是一个元组，</span></span><br><span class="line"><span class="comment"># 通常设置为 (0.9, 0.99)，表示较强的一阶矩估计（动量）和二阶矩估计（平方梯度）。</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># for logging results in a folder periodically</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.accelerator.is_main_process:</span><br><span class="line">            <span class="variable language_">self</span>.ema = EMA(diffusion_model, beta=ema_decay,</span><br><span class="line">                           update_every=ema_update_every)</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.set_results_folder(results_folder)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># step counter state</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># prepare model, dataloader, optimizer with accelerator</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.model, <span class="variable language_">self</span>.opt = <span class="variable language_">self</span>.accelerator.prepare(<span class="variable language_">self</span>.model, <span class="variable language_">self</span>.opt)</span><br><span class="line">        <span class="comment"># accelerator.prepare() 会将模型和优化器（以及任何其他支持的数据加载器或其他组件）准备好</span></span><br><span class="line">        <span class="comment"># ，并自动处理设备（如 cuda 或 cpu）之间的转移，使得代码可以在多设备训练时更加简洁和高效。</span></span><br><span class="line">        device = <span class="variable language_">self</span>.accelerator.device</span><br><span class="line">        <span class="variable language_">self</span>.device = device</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, milestone</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.accelerator.is_local_main_process:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;step&#x27;</span>: <span class="variable language_">self</span>.step,</span><br><span class="line">            <span class="string">&#x27;model&#x27;</span>: <span class="variable language_">self</span>.accelerator.get_state_dict(<span class="variable language_">self</span>.model),</span><br><span class="line">            <span class="string">&#x27;opt&#x27;</span>: <span class="variable language_">self</span>.opt.state_dict(),</span><br><span class="line">            <span class="string">&#x27;ema&#x27;</span>: <span class="variable language_">self</span>.ema.state_dict(),</span><br><span class="line">            <span class="string">&#x27;scaler&#x27;</span>: <span class="variable language_">self</span>.accelerator.scaler.state_dict() <span class="keyword">if</span> exists(<span class="variable language_">self</span>.accelerator.scaler) <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># data字典包含了当前训练的相关状态信息</span></span><br><span class="line">        torch.save(data, <span class="built_in">str</span>(<span class="variable language_">self</span>.results_folder / <span class="string">f&#x27;model-<span class="subst">&#123;milestone&#125;</span>.pt&#x27;</span>))</span><br><span class="line">        <span class="comment"># torch.save会将data字典序列化并写入指定路径的.pt文件</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, milestone</span>):</span><br><span class="line">        path = Path(<span class="variable language_">self</span>.results_folder / <span class="string">f&#x27;model-<span class="subst">&#123;milestone&#125;</span>.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> path.exists():</span><br><span class="line">            data = torch.load(</span><br><span class="line">                <span class="built_in">str</span>(path), map_location=<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">            model = <span class="variable language_">self</span>.accelerator.unwrap_model(<span class="variable language_">self</span>.model)</span><br><span class="line">            model.load_state_dict(data[<span class="string">&#x27;model&#x27;</span>])</span><br><span class="line"></span><br><span class="line">            <span class="variable language_">self</span>.step = data[<span class="string">&#x27;step&#x27;</span>]</span><br><span class="line">            <span class="variable language_">self</span>.opt.load_state_dict(data[<span class="string">&#x27;opt&#x27;</span>])</span><br><span class="line">            <span class="variable language_">self</span>.ema.load_state_dict(data[<span class="string">&#x27;ema&#x27;</span>])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> exists(<span class="variable language_">self</span>.accelerator.scaler) <span class="keyword">and</span> exists(data[<span class="string">&#x27;scaler&#x27;</span>]):</span><br><span class="line">                <span class="variable language_">self</span>.accelerator.scaler.load_state_dict(data[<span class="string">&#x27;scaler&#x27;</span>])</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;load model - &quot;</span>+<span class="built_in">str</span>(path))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.ema.to(<span class="variable language_">self</span>.device)</span><br><span class="line">    <span class="comment"># 加载模型：从磁盘加载模型的 state_dict，并将其恢复到当前的模型中。</span></span><br><span class="line">    <span class="comment"># 加载优化器和EMA：恢复优化器和 EMA 的状态，以便继续优化和稳定训练过程。</span></span><br><span class="line">    <span class="comment"># 恢复训练进度：通过恢复步数（step）继续训练。</span></span><br><span class="line">    <span class="comment"># 支持混合精度训练：如果使用了混合精度训练，恢复 scaler 的状态以便继续进行梯度缩放。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self</span>):</span><br><span class="line">        accelerator = <span class="variable language_">self</span>.accelerator</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tqdm(initial=<span class="variable language_">self</span>.step, total=<span class="variable language_">self</span>.train_num_steps, disable=<span class="keyword">not</span> accelerator.is_main_process) <span class="keyword">as</span> pbar:</span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="variable language_">self</span>.step &lt; <span class="variable language_">self</span>.train_num_steps:</span><br><span class="line"></span><br><span class="line">                total_loss = <span class="number">0.</span></span><br><span class="line">                <span class="comment"># 每gradient_accumulate_every个批次更新一次模型的参数</span></span><br><span class="line">                <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.gradient_accumulate_every):</span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">                        data = <span class="built_in">next</span>(<span class="variable language_">self</span>.dl)</span><br><span class="line">                        data = [item.to(<span class="variable language_">self</span>.device) <span class="keyword">for</span> item <span class="keyword">in</span> data]</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        data = <span class="built_in">next</span>(<span class="variable language_">self</span>.dl)</span><br><span class="line">                        data = data[<span class="number">0</span>] <span class="keyword">if</span> <span class="built_in">isinstance</span>(data, <span class="built_in">list</span>) <span class="keyword">else</span> data</span><br><span class="line">                        data = data.to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">with</span> <span class="variable language_">self</span>.accelerator.autocast():</span><br><span class="line">                        loss = <span class="variable language_">self</span>.model(data)</span><br><span class="line">                        loss = loss / <span class="variable language_">self</span>.gradient_accumulate_every</span><br><span class="line">                        total_loss = total_loss + loss.item()</span><br><span class="line"></span><br><span class="line">                    <span class="variable language_">self</span>.accelerator.backward(loss)</span><br><span class="line"></span><br><span class="line">                accelerator.clip_grad_norm_(<span class="variable language_">self</span>.model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">                <span class="comment"># 为了防止梯度爆炸，使用梯度裁剪（gradient clipping）。</span></span><br><span class="line">                <span class="comment"># 裁剪阈值设置为 1.0，即如果梯度的范数超过 1.0，会进行裁剪。</span></span><br><span class="line">                accelerator.wait_for_everyone()</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.opt.step()</span><br><span class="line">                <span class="comment"># 更新模型参数，通过优化器（self.opt）进行一次梯度更新。</span></span><br><span class="line">                <span class="variable language_">self</span>.opt.zero_grad()</span><br><span class="line">                <span class="comment"># 清空优化器中的梯度缓存。每个训练步骤后都需要手动清空梯度，以便下一个训练步骤计算新的梯度。</span></span><br><span class="line">                accelerator.wait_for_everyone()</span><br><span class="line"></span><br><span class="line">                <span class="variable language_">self</span>.step += <span class="number">1</span></span><br><span class="line">                <span class="keyword">if</span> accelerator.is_main_process:</span><br><span class="line">                    <span class="variable language_">self</span>.ema.to(<span class="variable language_">self</span>.device)</span><br><span class="line">                    <span class="variable language_">self</span>.ema.update()</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.step != <span class="number">0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.step % <span class="variable language_">self</span>.save_and_sample_every == <span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># 每 save_and_sample_every 步保存一次模型，并进行一次采样。</span></span><br><span class="line">                        milestone = <span class="variable language_">self</span>.step // <span class="variable language_">self</span>.save_and_sample_every</span><br><span class="line">                        <span class="variable language_">self</span>.sample(milestone)</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> <span class="variable language_">self</span>.step != <span class="number">0</span> <span class="keyword">and</span> <span class="variable language_">self</span>.step % (<span class="variable language_">self</span>.save_and_sample_every*<span class="number">10</span>) == <span class="number">0</span>:</span><br><span class="line">                            <span class="comment"># 每 save_and_sample_every * 10 步保存一次模型，以便定期保存更多的检查点。</span></span><br><span class="line">                            <span class="variable language_">self</span>.save(milestone)</span><br><span class="line"></span><br><span class="line">                pbar.set_description(<span class="string">f&#x27;loss: <span class="subst">&#123;total_loss:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">                pbar.update(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        accelerator.<span class="built_in">print</span>(<span class="string">&#x27;training complete&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, milestone, last=<span class="literal">False</span>, FID=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.ema.ema_model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="comment"># 将EMA（Exponential Moving Average）模型切换到评估模式。</span></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            batches = <span class="variable language_">self</span>.num_samples</span><br><span class="line">            <span class="comment"># 设置批次大小为 self.num_samples，即要生成的样本数量。</span></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.condition_type == <span class="number">0</span>:</span><br><span class="line">                x_input_sample = [<span class="number">0</span>]</span><br><span class="line">                show_x_input_sample = []</span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.condition_type == <span class="number">1</span>:</span><br><span class="line">                x_input_sample = [<span class="built_in">next</span>(<span class="variable language_">self</span>.sample_loader).to(<span class="variable language_">self</span>.device)]</span><br><span class="line">                show_x_input_sample = x_input_sample</span><br><span class="line">            <span class="comment"># condition——type为2的情况</span></span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.condition_type == <span class="number">2</span>:</span><br><span class="line">                x_input_sample = <span class="built_in">next</span>(<span class="variable language_">self</span>.sample_loader)</span><br><span class="line">                x_input_sample = [item.to(<span class="variable language_">self</span>.device)</span><br><span class="line">                                  <span class="keyword">for</span> item <span class="keyword">in</span> x_input_sample]</span><br><span class="line">                show_x_input_sample = x_input_sample</span><br><span class="line">                x_input_sample = x_input_sample[<span class="number">1</span>:]</span><br><span class="line">            <span class="keyword">elif</span> <span class="variable language_">self</span>.condition_type == <span class="number">3</span>:</span><br><span class="line">                x_input_sample = <span class="built_in">next</span>(<span class="variable language_">self</span>.sample_loader)</span><br><span class="line">                x_input_sample = [item.to(<span class="variable language_">self</span>.device)</span><br><span class="line">                                  <span class="keyword">for</span> item <span class="keyword">in</span> x_input_sample]</span><br><span class="line">                show_x_input_sample = x_input_sample</span><br><span class="line">                x_input_sample = x_input_sample[<span class="number">1</span>:]</span><br><span class="line">        <span class="comment"># 当 self.condition_type == 2 时，代码从 sample_loader 中获取一个批次的数据，将其移到设备上，</span></span><br><span class="line">        <span class="comment"># 并通过切片操作丢弃其中的第一个元素。</span></span><br><span class="line">        <span class="comment"># 最终，x_input_sample 只保留了去除条件输入后的数据，</span></span><br><span class="line">        <span class="comment"># 通常是目标图像或标签（具体根据任务而定）</span></span><br><span class="line">            all_images_list = show_x_input_sample + \</span><br><span class="line">                <span class="built_in">list</span>(<span class="variable language_">self</span>.ema.ema_model.sample(</span><br><span class="line">                    x_input_sample, batch_size=batches, last=last))</span><br><span class="line"></span><br><span class="line">            all_images = torch.cat(all_images_list, dim=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># all_images_list 将条件样本（show_x_input_sample）和</span></span><br><span class="line">        <span class="comment"># 通过EMA模型生成的样本（self.ema.ema_model.sample(...)）</span></span><br><span class="line">        <span class="comment"># 合并为一个列表。</span></span><br><span class="line">        <span class="comment"># torch.cat 将这个列表中的所有张量沿批次维度（dim=0）拼接在一起，</span></span><br><span class="line">        <span class="comment"># 得到一个包含所有样本的大的张量 all_images。</span></span><br><span class="line">            <span class="keyword">if</span> last:</span><br><span class="line">                nrow = <span class="built_in">int</span>(math.sqrt(<span class="variable language_">self</span>.num_samples))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                nrow = all_images.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> FID:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batches):</span><br><span class="line">                    file_name = <span class="string">f&#x27;sample-<span class="subst">&#123;milestone&#125;</span>.png&#x27;</span></span><br><span class="line">                    utils.save_image(</span><br><span class="line">                        all_images_list[<span class="number">0</span>][i].unsqueeze(<span class="number">0</span>), os.path.join(<span class="variable language_">self</span>.results_folder, file_name), nrow=<span class="number">1</span>)</span><br><span class="line">                    milestone += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> milestone&gt;=<span class="variable language_">self</span>.total_n_samples:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                file_name = <span class="string">f&#x27;sample-<span class="subst">&#123;milestone&#125;</span>.png&#x27;</span></span><br><span class="line">                utils.save_image(all_images, <span class="built_in">str</span>(</span><br><span class="line">                    <span class="variable language_">self</span>.results_folder / file_name), nrow=nrow)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;sampe-save &quot;</span>+file_name)</span><br><span class="line">        <span class="keyword">return</span> milestone</span><br><span class="line"><span class="comment"># 如果 FID 为 True，代码将会将每个生成的图像分别保存，</span></span><br><span class="line"><span class="comment"># 并且每次保存时递增 milestone，直到达到指定的样本数量 total_n_samples。</span></span><br><span class="line"><span class="comment"># 保存的文件名包含了当前的 milestone，这是用来标识保存的图像对应的训练步数。</span></span><br><span class="line"><span class="comment"># 最终返回更新后的 milestone，便于后续继续保存样本。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">self, sample=<span class="literal">False</span>, last=<span class="literal">True</span>, FID=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test start&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">            <span class="variable language_">self</span>.ema.ema_model.<span class="built_in">eval</span>()</span><br><span class="line">            loader = DataLoader(</span><br><span class="line">                dataset=<span class="variable language_">self</span>.sample_dataset,</span><br><span class="line">                batch_size=<span class="number">1</span>)</span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> items <span class="keyword">in</span> loader:</span><br><span class="line">                <span class="keyword">if</span> <span class="variable language_">self</span>.condition:</span><br><span class="line">                    file_name = <span class="variable language_">self</span>.sample_dataset.load_name(</span><br><span class="line">                        i, sub_dir=<span class="variable language_">self</span>.sub_dir)</span><br><span class="line">                    <span class="comment"># 如果启用了条件生成，就加载样本数据集每次从数据集中获取</span></span><br><span class="line">                    <span class="comment"># 一个图像样本</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    file_name = <span class="string">f&#x27;<span class="subst">&#123;i&#125;</span>.png&#x27;</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    batches = <span class="variable language_">self</span>.num_samples</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> <span class="variable language_">self</span>.condition_type == <span class="number">0</span>:</span><br><span class="line">                        x_input_sample = [<span class="number">0</span>]</span><br><span class="line">                        show_x_input_sample = []</span><br><span class="line">                    <span class="keyword">elif</span> <span class="variable language_">self</span>.condition_type == <span class="number">1</span>:</span><br><span class="line">                        x_input_sample = [items.to(<span class="variable language_">self</span>.device)]</span><br><span class="line">                        show_x_input_sample = x_input_sample</span><br><span class="line">                    <span class="keyword">elif</span> <span class="variable language_">self</span>.condition_type == <span class="number">2</span>:</span><br><span class="line">                        x_input_sample = [item.to(<span class="variable language_">self</span>.device)</span><br><span class="line">                                          <span class="keyword">for</span> item <span class="keyword">in</span> items]</span><br><span class="line">                        show_x_input_sample = x_input_sample</span><br><span class="line">                        x_input_sample = x_input_sample[<span class="number">1</span>:]</span><br><span class="line">                    <span class="keyword">elif</span> <span class="variable language_">self</span>.condition_type == <span class="number">3</span>:</span><br><span class="line">                        x_input_sample = [item.to(<span class="variable language_">self</span>.device)</span><br><span class="line">                                          <span class="keyword">for</span> item <span class="keyword">in</span> items]</span><br><span class="line">                        show_x_input_sample = x_input_sample</span><br><span class="line">                        x_input_sample = x_input_sample[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">if</span> sample:</span><br><span class="line">                        all_images_list = show_x_input_sample + \</span><br><span class="line">                            <span class="built_in">list</span>(<span class="variable language_">self</span>.ema.ema_model.sample(</span><br><span class="line">                                x_input_sample, batch_size=batches))</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        all_images_list = <span class="built_in">list</span>(<span class="variable language_">self</span>.ema.ema_model.sample(</span><br><span class="line">                            x_input_sample, batch_size=batches, last=last))</span><br><span class="line">                        all_images_list = [all_images_list[-<span class="number">1</span>]]</span><br><span class="line">                        <span class="keyword">if</span> <span class="variable language_">self</span>.crop_patch:</span><br><span class="line">                            k = <span class="number">0</span></span><br><span class="line">                            <span class="keyword">for</span> img <span class="keyword">in</span> all_images_list:</span><br><span class="line">                                pad_size = <span class="variable language_">self</span>.sample_dataset.get_pad_size(i)</span><br><span class="line">                                _, _, h, w = img.shape</span><br><span class="line">                                img = img[:, :, <span class="number">0</span>:h -</span><br><span class="line">                                          pad_size[<span class="number">0</span>], <span class="number">0</span>:w-pad_size[<span class="number">1</span>]]</span><br><span class="line">                                all_images_list[k] = img</span><br><span class="line">                                k += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                all_images = torch.cat(all_images_list, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> last:</span><br><span class="line">                    nrow = <span class="built_in">int</span>(math.sqrt(<span class="variable language_">self</span>.num_samples))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    nrow = all_images.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">                utils.save_image(all_images, <span class="built_in">str</span>(</span><br><span class="line">                    <span class="variable language_">self</span>.results_folder / file_name), nrow=nrow)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;test-save &quot;</span>+file_name)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> FID:</span><br><span class="line">                <span class="variable language_">self</span>.total_n_samples = <span class="number">50000</span></span><br><span class="line">                img_id = <span class="built_in">len</span>(glob.glob(<span class="string">f&quot;<span class="subst">&#123;self.results_folder&#125;</span>/*&quot;</span>))</span><br><span class="line">                n_rounds = (<span class="variable language_">self</span>.total_n_samples - img_id) // <span class="variable language_">self</span>.num_samples+<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                n_rounds = <span class="number">100</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_rounds):</span><br><span class="line">                <span class="keyword">if</span> FID:</span><br><span class="line">                    i = img_id</span><br><span class="line">                img_id = <span class="variable language_">self</span>.sample(i, last=last, FID=FID)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;test end&quot;</span>)</span><br><span class="line"><span class="comment"># 如果需要计算 FID，它会生成 50,000 个样本，</span></span><br><span class="line"><span class="comment"># 并根据已生成样本的数量来确定剩余需要生成多少图像。</span></span><br><span class="line"><span class="comment"># 如果不计算 FID，则默认生成 100 轮样本。</span></span><br><span class="line"><span class="comment"># 生成图像的数量由每轮生成的样本数 self.num_samples 和总样本数 self.total_n_samples 控制。</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">set_results_folder</span>(<span class="params">self, path</span>):</span><br><span class="line">        <span class="variable language_">self</span>.results_folder = Path(path)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="variable language_">self</span>.results_folder.exists():</span><br><span class="line">            os.makedirs(<span class="variable language_">self</span>.results_folder)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="模型图"><a href="#模型图" class="headerlink" title="模型图"></a>模型图</h2><h3 id="Block"><a href="#Block" class="headerlink" title="Block"></a>Block</h3><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411201656092.png" alt="image-20241120165659032"></p>
<h3 id="ResnetBlock"><a href="#ResnetBlock" class="headerlink" title="ResnetBlock"></a>ResnetBlock</h3><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411201657343.png" alt="image-20241120165723283"></p>
<h3 id="Unet"><a href="#Unet" class="headerlink" title="Unet"></a>Unet</h3><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411201658558.png" alt="image-20241120165842474"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411201658444.png" alt="image-20241120165852383"></p>
<h3 id="train-detail"><a href="#train-detail" class="headerlink" title="train_detail"></a>train_detail</h3><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411202151946.png" alt="image-20241120215120889"></p>
<p>$$ I_{t-1} &#x3D; I_t - \left( \bar{\alpha}<em>t - \bar{\alpha}</em>{t-1} \right) I_{res}^\theta - \left( \bar{\beta}<em>t - \bar{\beta}</em>{t-1} \right) \epsilon_\theta. $$</p>
<p>$$ I_t &#x3D; I_{in} + \left( \bar{\alpha}<em>t - 1 \right) I</em>{res} + \bar{\beta}_t \epsilon. $$</p>
<p>$$ I_{res} &#x3D; Img_{res} - I_t$$</p>
<h2 id="实验代码损失函数"><a href="#实验代码损失函数" class="headerlink" title="实验代码损失函数"></a>实验代码损失函数</h2><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412091556245.png" alt="image-20241209155614195"></p>
<blockquote>
<p>目前走的都是l1损失</p>
</blockquote>
<h2 id="生成时间步系数解读"><a href="#生成时间步系数解读" class="headerlink" title="生成时间步系数解读"></a>生成时间步系数解读</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def <span class="title function_">gen_coefficients</span><span class="params">(timesteps, schedule=<span class="string">&quot;increased&quot;</span>, sum_scale=<span class="number">1</span>)</span>:</span><br><span class="line">    # 用于生成时间步的系数，根据传入的schedule参数，会生成不同的时间步权重（alphas）</span><br><span class="line">    <span class="keyword">if</span> schedule == <span class="string">&quot;increased&quot;</span>:</span><br><span class="line">        # 逐步增加的权重</span><br><span class="line">        x = torch.linspace(<span class="number">1</span>, timesteps, timesteps, dtype=torch.float64)</span><br><span class="line">        # 生成一个<span class="number">1</span>到timesteps的等差数列，包含timesteps个元素</span><br><span class="line">        scale = <span class="number">0.5</span>*timesteps*(timesteps+<span class="number">1</span>)</span><br><span class="line">        # 缩放因子</span><br><span class="line">        alphas = x/scale</span><br><span class="line">        # 归一化后的系数列表</span><br><span class="line">    elif schedule == <span class="string">&quot;decreased&quot;</span>:</span><br><span class="line">        # # 逐步减少的权重</span><br><span class="line">        x = torch.linspace(<span class="number">1</span>, timesteps, timesteps, dtype=torch.float64)</span><br><span class="line">        x = torch.flip(x, dims=[<span class="number">0</span>])</span><br><span class="line">        scale = <span class="number">0.5</span>*timesteps*(timesteps+<span class="number">1</span>)</span><br><span class="line">        alphas = x/scale</span><br><span class="line">    elif schedule == <span class="string">&quot;average&quot;</span>:</span><br><span class="line">        # 均匀的参数</span><br><span class="line">        alphas = torch.full([timesteps], <span class="number">1</span>/timesteps, dtype=torch.float64)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        alphas = torch.full([timesteps], <span class="number">1</span>/timesteps, dtype=torch.float64)</span><br><span class="line">    <span class="keyword">assert</span> alphas.sum()-torch.tensor(<span class="number">1</span>) &lt; torch.tensor(<span class="number">1e-10</span>)</span><br><span class="line">    # 确保生成的系数总和非常接近<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> alphas*sum_scale</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">def <span class="title function_">gen_coefficients</span><span class="params">(timesteps, schedule=<span class="string">&quot;increased&quot;</span>, sum_scale=<span class="number">1</span>)</span>:</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>timesteps是时间步的数量</p>
<p>sum_scale缩放因子</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> schedule == <span class="string">&quot;increased&quot;</span>:</span><br><span class="line">    x = torch.linspace(<span class="number">1</span>, timesteps, timesteps, dtype=torch.float64)</span><br><span class="line">    scale = <span class="number">0.5</span>*timesteps*(timesteps+<span class="number">1</span>)</span><br><span class="line">    alphas = x/scale <span class="comment"># 归一化</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<blockquote>
<p>x是一个切割好的数据权重，</p>
<p><code>torch.linspace(1, timesteps, timesteps, dtype=torch.float64)</code> 生成一个从 1 到 <code>timesteps</code> 的等差数列。这个数列是逐步递增的。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">alphas = gen_coefficients(timesteps, schedule=<span class="string">&quot;decreased&quot;</span>)</span><br><span class="line"><span class="comment"># 表示图像在扩撒过程中的渐变系数,控制每个时间步的衰减程度</span></span><br><span class="line">alphas_cumsum = alphas.cumsum(dim=<span class="number">0</span>).clip(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 是alphas的累加和,用于表示每个时间步中,图像的衰减程度</span></span><br><span class="line"><span class="comment"># alphas.cumsum(dim=0)：计算 alphas 的累积和。dim=0 表示沿着第一个维度（即时间步的维度）计算累积和。举个例子，假设 alphas = [0.2, 0.3, 0.5]，那么累积和的结果是 [0.2, 0.5, 1.0]。</span></span><br><span class="line">alphas_cumsum_prev = F.pad(alphas_cumsum[:-<span class="number">1</span>], (<span class="number">1</span>, <span class="number">0</span>), value=<span class="number">1.</span>)</span><br></pre></td></tr></table></figure>

<p>**<code>F.pad(..., (1, 0), value=1.)</code>**：<code>F.pad</code> 用来对 <code>alphas_cumsum[:-1]</code> 进行填充，<code>(1, 0)</code> 表示在序列的左边填充 1 个元素，在右边不填充任何元素。<code>value=1.</code> 指定填充值为 1。这样，如果原始的 <code>alphas_cumsum[:-1]</code> 是 <code>[0.2, 0.5]</code>，则填充后的结果是 <code>[1.0, 0.2, 0.5]</code>。</p>
<p>填充后的 &#x3D;&#x3D;<code>alphas_cumsum_prev</code> 表示的是<strong>前一个时间步的衰减程度</strong>。&#x3D;&#x3D;在扩散过程中的每个时间步，&#x3D;&#x3D;当前时间步的衰减程度是通过当前时间步的 <code>alphas_cumsum</code> 和前一个时间步的 <code>alphas_cumsum</code> 之间的差值来计算的。&#x3D;&#x3D;因此，填充操作是为了将前一个时间步的累积和设置为 1（即初始时的完全保留状态）。这在扩散模型中是常见的操作，用于初始化。</p>
<h2 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h2><h3 id="将test阶段生成的图片都改成-png"><a href="#将test阶段生成的图片都改成-png" class="headerlink" title="将test阶段生成的图片都改成.png"></a>将test阶段生成的图片都改成.png</h3><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412092013273.png" alt="image-20241209201334199"></p>
<blockquote>
<p>修改test最后的输出</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412092113433.png" alt="image-20241209211302383"></p>
<h3 id="损失更新成l1-msssim"><a href="#损失更新成l1-msssim" class="headerlink" title="损失更新成l1+msssim"></a>损失更新成l1+msssim</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MSSSIML1Loss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, C1=<span class="number">0.01</span>, C2=<span class="number">0.03</span>, sigma=(<span class="params"><span class="number">0.5</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">4.</span>, <span class="number">8.</span></span>), alpha=<span class="number">0.025</span>, reduction=<span class="string">&#x27;mean&#x27;</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MSSSIML1Loss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="variable language_">self</span>.C1 = C1 ** <span class="number">2</span>  <span class="comment"># C1 的平方</span></span><br><span class="line">        <span class="variable language_">self</span>.C2 = C2 ** <span class="number">2</span>  <span class="comment"># C2 的平方</span></span><br><span class="line">        <span class="variable language_">self</span>.sigma = sigma  <span class="comment"># 高斯滤波器的标准差</span></span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha  <span class="comment"># MSSSIM 和 L1 损失的权重比例</span></span><br><span class="line">        <span class="variable language_">self</span>.reduction = reduction  <span class="comment"># 损失的归约方式</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, output, target</span>):</span><br><span class="line">        <span class="comment"># 获取 batch_size、channels 和宽高</span></span><br><span class="line">        batch_size, channels, width, height = output.shape</span><br><span class="line"></span><br><span class="line">        num_scale = <span class="built_in">len</span>(<span class="variable language_">self</span>.sigma)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化一些中间变量</span></span><br><span class="line">        w = []  <span class="comment"># 存储每个尺度的高斯权重</span></span><br><span class="line">        mux = []  <span class="comment"># 存储每个尺度的均值</span></span><br><span class="line">        muy = []  <span class="comment"># 存储每个尺度的目标均值</span></span><br><span class="line">        sigmax2 = []  <span class="comment"># 存储每个尺度的方差</span></span><br><span class="line">        sigmay2 = []  <span class="comment"># 存储每个尺度的目标方差</span></span><br><span class="line">        sigmaxy = []  <span class="comment"># 存储每个尺度的协方差</span></span><br><span class="line">        l = []  <span class="comment"># 存储每个尺度的 L</span></span><br><span class="line">        cs = []  <span class="comment"># 存储每个尺度的 C</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 为每个尺度构造高斯滤波器</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_scale):</span><br><span class="line">            <span class="comment"># 计算高斯滤波器的宽度和高度，使其与输入图像大小一致</span></span><br><span class="line">            size = width  <span class="comment"># 或者 height, 这里只需要确保宽度和高度一致</span></span><br><span class="line">            gaussian = np.exp(-np.arange(-(size//<span class="number">2</span>), (size//<span class="number">2</span>))**<span class="number">2</span> / (<span class="number">2</span> * <span class="variable language_">self</span>.sigma[i]**<span class="number">2</span>))</span><br><span class="line">            gaussian = np.outer(gaussian, gaussian)  <span class="comment"># 生成二维高斯核</span></span><br><span class="line">            gaussian /= np.<span class="built_in">sum</span>(gaussian)  <span class="comment"># 归一化</span></span><br><span class="line">                <span class="comment"># 裁剪高斯核至 256x256</span></span><br><span class="line">            gaussian = torch.tensor(gaussian, dtype=torch.float32).unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)  <span class="comment"># 转换为 Tensor 并调整形状</span></span><br><span class="line">            gaussian = gaussian.repeat(batch_size, channels, <span class="number">1</span>, <span class="number">1</span>)  <span class="comment"># 重复到 batch 和 channel 维度</span></span><br><span class="line">            w.append(gaussian)</span><br><span class="line">        </span><br><span class="line">        w = torch.stack(w, dim=<span class="number">0</span>)  <span class="comment"># 将多个尺度的高斯权重堆叠起来</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确保高斯权重 w 的尺寸是 (num_scale, batch_size, channels, width, height)</span></span><br><span class="line">        <span class="comment"># 将输出和目标数据扩展到多尺度</span></span><br><span class="line">        output = output.unsqueeze(<span class="number">0</span>).repeat(num_scale, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        target = target.unsqueeze(<span class="number">0</span>).repeat(num_scale, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        output = output.to(device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">        w = w.to(device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算每个尺度的均值和方差</span></span><br><span class="line">        mux = torch.<span class="built_in">sum</span>(w * output, dim=(<span class="number">3</span>, <span class="number">4</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        muy = torch.<span class="built_in">sum</span>(w * target, dim=(<span class="number">3</span>, <span class="number">4</span>), keepdim=<span class="literal">True</span>)</span><br><span class="line">        sigmax2 = torch.<span class="built_in">sum</span>(w * output**<span class="number">2</span>, dim=(<span class="number">3</span>, <span class="number">4</span>), keepdim=<span class="literal">True</span>) - mux**<span class="number">2</span></span><br><span class="line">        sigmay2 = torch.<span class="built_in">sum</span>(w * target**<span class="number">2</span>, dim=(<span class="number">3</span>, <span class="number">4</span>), keepdim=<span class="literal">True</span>) - muy**<span class="number">2</span></span><br><span class="line">        sigmaxy = torch.<span class="built_in">sum</span>(w * output * target, dim=(<span class="number">3</span>, <span class="number">4</span>), keepdim=<span class="literal">True</span>) - mux * muy</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 L 和 C</span></span><br><span class="line">        l = (<span class="number">2</span> * mux * muy + <span class="variable language_">self</span>.C1) / (mux**<span class="number">2</span> + muy**<span class="number">2</span> + <span class="variable language_">self</span>.C1)</span><br><span class="line">        cs = (<span class="number">2</span> * sigmaxy + <span class="variable language_">self</span>.C2) / (sigmax2 + sigmay2 + <span class="variable language_">self</span>.C2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算所有通道的 C 的乘积</span></span><br><span class="line">        Pcs = torch.prod(cs, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算 MSSSIM 损失</span></span><br><span class="line">        loss_MSSSIM = <span class="number">1</span> - torch.<span class="built_in">sum</span>(l[-<span class="number">1</span>, :, :, :, :] * Pcs) / (batch_size * channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算带有高斯加权的 L1 损失</span></span><br><span class="line">        diff = output - target</span><br><span class="line">        loss_L1 = torch.<span class="built_in">sum</span>(torch.<span class="built_in">abs</span>(diff) * w[-<span class="number">1</span>, :, :, :, :]) / (batch_size * channels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终损失：MSSSIM 和 L1 损失的加权和</span></span><br><span class="line">        loss = <span class="variable language_">self</span>.alpha * loss_MSSSIM + (<span class="number">1</span> - <span class="variable language_">self</span>.alpha) * loss_L1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据 reduction 参数决定如何返回损失</span></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.reduction == <span class="string">&#x27;mean&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> loss.mean()  <span class="comment"># 返回平均损失</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.reduction == <span class="string">&#x27;sum&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> loss.<span class="built_in">sum</span>()  <span class="comment"># 返回损失和</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="variable language_">self</span>.reduction == <span class="string">&#x27;none&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> loss  <span class="comment"># 返回每个元素的损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 示例用法</span></span><br><span class="line"><span class="comment"># if __name__ == &quot;__main__&quot;:</span></span><br><span class="line"><span class="comment">#     # 示例输入张量 (batch_size, channels, width, height)</span></span><br><span class="line"><span class="comment">#     output = torch.randn(1, 3, 256, 256)  # 1 个样本，3 个通道，256x256 的图像</span></span><br><span class="line"><span class="comment">#     target = torch.randn(1, 3, 256, 256)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     loss_fn = MSSSIML1Loss(reduction=&#x27;mean&#x27;)  # 创建损失函数实例，设置 reduction=&#x27;mean&#x27;</span></span><br><span class="line"><span class="comment">#     loss = loss_fn(output, target)  # 计算损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     print(f&#x27;MSSSIML1 Loss: &#123;loss.item()&#125;&#x27;)  # 输出损失值</span></span><br></pre></td></tr></table></figure>

<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412161638361.png" alt="image-20241216163810235"></p>
<h2 id="中期实现点"><a href="#中期实现点" class="headerlink" title="中期实现点"></a>中期实现点</h2><h3 id="1-训练过程中的根据梯度来进行残差和噪声的权重分配"><a href="#1-训练过程中的根据梯度来进行残差和噪声的权重分配" class="headerlink" title="1.训练过程中的根据梯度来进行残差和噪声的权重分配"></a>1.训练过程中的根据梯度来进行残差和噪声的权重分配</h3><blockquote>
<p>残差的定义</p>
</blockquote>
<p>给定一对数据（<code>x_i</code>, <code>y_i</code>），模型的预测值为 <code>ŷ_i = f(x_i)</code>，其中 <code>f(x_i)</code> 是模型对输入 <code>x_i</code> 的预测值，<code>y_i</code> 是真实标签。残差 <code>r_i</code> 定义为：</p>
<p>$$                   ri&#x3D;yi−\hat{y}_i&#x3D;yi−f(x_i) $$</p>
<p><strong>以均方误差（MSE）为例：</strong></p>
<p>$$ L &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i-\hat{y}<em>i)^2&#x3D;\frac{1}{n}\sum</em>{i&#x3D;1}^{n}r_i^2$$</p>
<p>梯度 $$ 2r_i $$</p>
<p>最终贡献以残差的归一化值为其参数</p>
<h3 id="2-模型自适应处理雨雪雾图像"><a href="#2-模型自适应处理雨雪雾图像" class="headerlink" title="2.模型自适应处理雨雪雾图像"></a>2.模型自适应处理雨雪雾图像</h3><blockquote>
<p>雪花的遮盖效应更强，雨雾虽然可能会对视觉造成影响，但是相比我们之后所做的mask直接剪裁掉出现黑影情况下，直接进行残差恢复效果更好</p>
</blockquote>
<p>预训练掩码生成器</p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171115989.png" alt="image-20241217111511932"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171115938.png" alt="image-20241217111523889"></p>
<p>原始的mask捕捉器</p>
<p>SA对特征矩阵中的每个元素进行平方操作,可以突出激活值比较高的区域(255为白色这种),并抑制激活值比较低的区域</p>
<p>CA通过两个并行的卷积单元对特征矩阵进行操作,进行哈达玛积,相当于将同一个元素复制了两份进行乘积,大的更大,小的更小</p>
<p>目前想找一个模块替换CA,选定使用&#x3D;&#x3D;CBAM&#x3D;&#x3D;(轻量的卷积注意力模块)</p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171319468.png" alt="image-20241217131922395"></p>
<p>包含CAM和SAM两个子模块,分别进行通道和空间上的Attention</p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171325063.png" alt="image-20241217132533016"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171325039.png" alt="image-20241217132550984"></p>
<p>​	将输入的feature map经过两个并行的MaxPool层和AvgPool层，将特征图从C<em>H</em>W变为C<em>1</em>1的大小，然后经过Share MLP模块，在该模块中，它先将通道数压缩为原来的1&#x2F;r（Reduction，减少率）倍，再扩张到原通道数，经过ReLU激活函数得到两个激活后的结果。将这两个输出结果进行逐元素相加，再通过一个sigmoid激活函数得到Channel Attention的输出结果，再将这个输出结果乘原图，变回C<em>H</em>W的大小。<br><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171334551.png" alt="image-20241217133415473"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171334779.png" alt="image-20241217133450723"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171335870.png" alt="image-20241217133504818"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171336930.png" alt="image-20241217133611865"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171338101.png" alt="image-20241217133816050"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171347451.png" alt="image-20241217134704401"></p>
<p><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202412171347063.png" alt="image-20241217134748981"></p>
<table>
<thead>
<tr>
<th>DataSet</th>
<th>PSNR</th>
<th>SSIM</th>
<th>Model_Name</th>
</tr>
</thead>
<tbody><tr>
<td>Rain100H</td>
<td>26.35</td>
<td>0.8287</td>
<td>RDDM</td>
</tr>
<tr>
<td></td>
<td><strong>30.38</strong></td>
<td><strong>0.8939</strong></td>
<td>Adaptive_RDDM</td>
</tr>
<tr>
<td>Rain100L</td>
<td>32.48</td>
<td>0.9023</td>
<td>RDDM</td>
</tr>
<tr>
<td></td>
<td><strong>33.57</strong></td>
<td><strong>0.9307</strong></td>
<td>Adaptive_RDDM</td>
</tr>
<tr>
<td>RESIDE(SOTS-OutDoor)</td>
<td>27.86</td>
<td>0.8208</td>
<td>RDDM</td>
</tr>
<tr>
<td></td>
<td><strong>28.89</strong></td>
<td><strong>0.8867</strong></td>
<td>Adaptive_RDDM</td>
</tr>
<tr>
<td>RESIDE(HSTS)</td>
<td>30.75</td>
<td>0.8780</td>
<td>RDDM</td>
</tr>
<tr>
<td></td>
<td><strong>32.14</strong></td>
<td><strong>0.9240</strong></td>
<td>Adaptive_RDDM</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>DataSet</th>
<th>PSNR</th>
<th>SSIM</th>
<th>Model_Name</th>
</tr>
</thead>
<tbody><tr>
<td>CSD</td>
<td>28.28</td>
<td>0.6697</td>
<td>RDDM</td>
</tr>
<tr>
<td></td>
<td>29.99</td>
<td>0.7871</td>
<td>Adaptive_RDDM</td>
</tr>
<tr>
<td></td>
<td><strong>31.27</strong></td>
<td><strong>0.8823</strong></td>
<td>UnMask_Adaptive_RDDM</td>
</tr>
</tbody></table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://nil0330.github.io">tfy</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://nil0330.github.io/2024/11/19/%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/">https://nil0330.github.io/2024/11/19/%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://nil0330.github.io" target="_blank">零度Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/">源码解读</a></div><div class="post-share"><div class="social-share" data-image="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411141618792.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/11/20/%E6%9C%89%E8%B6%A3%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/" title="有趣的学习博客"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">有趣的学习博客</div></div><div class="info-2"><div class="info-item-1">vimvim学习博客vim学习资料 雨月空间博客 </div></div></div></a><a class="pagination-related" href="/2024/11/14/learn-github/" title="learn_github"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">learn_github</div></div><div class="info-2"><div class="info-item-1">github学习gitloggit log --abbrev-commit   只显示足够多的字符来标识提交  git log --pretty=online   不想看到有关作者和日期的消息  git log --pretty=online --abbrev-commit   组合使用  git log --online  网站设计网站图标# Favicon（网站图）favicon: /img/favicon.png   网站图标    头像    主页封面图片      </div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="https://raw.githubusercontent.com/nil0330/PicGo1/main/202411141618792.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">tfy</div><div class="author-info-description">心中有沟壑、眉目做山河</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">6</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://github.com/nil0330/" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:2353570949@qq.com" target="_blank" title="Email"><i class="fas fa-envelope-open-text"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%9B%B8%E5%85%B3%E7%BB%86%E8%8A%82"><span class="toc-number">1.</span> <span class="toc-text">实验相关细节</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81"><span class="toc-number">1.1.</span> <span class="toc-text">实验代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%9B%BE"><span class="toc-number">1.2.</span> <span class="toc-text">模型图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Block"><span class="toc-number">1.2.1.</span> <span class="toc-text">Block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ResnetBlock"><span class="toc-number">1.2.2.</span> <span class="toc-text">ResnetBlock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unet"><span class="toc-number">1.2.3.</span> <span class="toc-text">Unet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#train-detail"><span class="toc-number">1.2.4.</span> <span class="toc-text">train_detail</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.3.</span> <span class="toc-text">实验代码损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%97%B6%E9%97%B4%E6%AD%A5%E7%B3%BB%E6%95%B0%E8%A7%A3%E8%AF%BB"><span class="toc-number">1.4.</span> <span class="toc-text">生成时间步系数解读</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%AE%E6%94%B9"><span class="toc-number">1.5.</span> <span class="toc-text">修改</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86test%E9%98%B6%E6%AE%B5%E7%94%9F%E6%88%90%E7%9A%84%E5%9B%BE%E7%89%87%E9%83%BD%E6%94%B9%E6%88%90-png"><span class="toc-number">1.5.1.</span> <span class="toc-text">将test阶段生成的图片都改成.png</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E6%9B%B4%E6%96%B0%E6%88%90l1-msssim"><span class="toc-number">1.5.2.</span> <span class="toc-text">损失更新成l1+msssim</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">1.5.2.1.</span> <span class="toc-text">实验结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%AD%E6%9C%9F%E5%AE%9E%E7%8E%B0%E7%82%B9"><span class="toc-number">1.6.</span> <span class="toc-text">中期实现点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E6%A0%B9%E6%8D%AE%E6%A2%AF%E5%BA%A6%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%AE%8B%E5%B7%AE%E5%92%8C%E5%99%AA%E5%A3%B0%E7%9A%84%E6%9D%83%E9%87%8D%E5%88%86%E9%85%8D"><span class="toc-number">1.6.1.</span> <span class="toc-text">1.训练过程中的根据梯度来进行残差和噪声的权重分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%87%AA%E9%80%82%E5%BA%94%E5%A4%84%E7%90%86%E9%9B%A8%E9%9B%AA%E9%9B%BE%E5%9B%BE%E5%83%8F"><span class="toc-number">1.6.2.</span> <span class="toc-text">2.模型自适应处理雨雪雾图像</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/21/%E5%AE%9D%E7%9A%84experiment/" title="宝的experiment">宝的experiment</a><time datetime="2024-11-21T13:28:19.000Z" title="发表于 2024-11-21 21:28:19">2024-11-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/20/%E6%9C%89%E8%B6%A3%E7%9A%84%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/" title="有趣的学习博客">有趣的学习博客</a><time datetime="2024-11-20T14:24:06.000Z" title="发表于 2024-11-20 22:24:06">2024-11-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/19/%E5%AE%9E%E9%AA%8C%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/" title="实验代码解读">实验代码解读</a><time datetime="2024-11-19T08:44:21.000Z" title="发表于 2024-11-19 16:44:21">2024-11-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/14/learn-github/" title="learn_github">learn_github</a><time datetime="2024-11-14T01:57:26.000Z" title="发表于 2024-11-14 09:57:26">2024-11-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/11/04/%E9%9D%A2%E8%AF%95test/" title="面试test">面试test</a><time datetime="2024-11-04T12:27:20.000Z" title="发表于 2024-11-04 20:27:20">2024-11-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2024 By tfy</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/fireworks.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>